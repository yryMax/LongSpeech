{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-09T06:51:24.306169Z",
     "start_time": "2025-07-09T06:51:24.292809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import count\n",
    "import heapq\n",
    "import soundfile as sf\n",
    "import librosa\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:31:16.942094Z",
     "start_time": "2025-07-09T06:31:08.137814Z"
    }
   },
   "cell_type": "code",
   "source": "from LibriSpeechEntity import LibriSpeechEntity",
   "id": "c466707a6f1b3314",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renyi/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:31:38.510702Z",
     "start_time": "2025-07-09T06:31:38.506306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INPUTDIR = \"/mnt/d/voicedata/test-clean/LibriSpeech/test-clean\"\n",
    "N_WORKERS = os.cpu_count() or 4"
   ],
   "id": "85a549c031318ee9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:31:39.887915Z",
     "start_time": "2025-07-09T06:31:39.883335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "speakerchapter_dict = {}\n",
    "OUT_DIR = '../datasets/LongSpeech'"
   ],
   "id": "65f27d2990c5015f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:31:41.456435Z",
     "start_time": "2025-07-09T06:31:41.444646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# read from metadata\n",
    "config = json.load(open(os.path.join(OUT_DIR, 'metadata.json')))\n",
    "\n",
    "AVG_DURATION = config['avg_duration']\n",
    "SAMPLE_RATE = config['sample_rate']\n",
    "OUT_FILE_NAME = config['source']\n",
    "\n",
    "counter = count(start=config['amount'])"
   ],
   "id": "7e1c4fa7df2f044d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:32:22.625691Z",
     "start_time": "2025-07-09T06:31:43.284405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for speakers in os.listdir(INPUTDIR):\n",
    "    for chapter in os.listdir(os.path.join(INPUTDIR, speakers)):\n",
    "        speaker_chapter_key = speakers + '#' + chapter\n",
    "        duration = 0\n",
    "        segment_amount = 0\n",
    "        for wavs in os.listdir(os.path.join(INPUTDIR, speakers, chapter)):\n",
    "\n",
    "            if not wavs.endswith(\".flac\"):\n",
    "                continue\n",
    "\n",
    "            audio_path = os.path.join(INPUTDIR, speakers, chapter, wavs)\n",
    "\n",
    "            info = sf.info(audio_path)\n",
    "            dur  = info.frames / info.samplerate\n",
    "            duration += dur\n",
    "            segment_amount += 1\n",
    "        speakerchapter_dict[speaker_chapter_key] = (duration, 0, segment_amount)\n",
    "\n",
    "\n",
    "\n",
    "print(speakerchapter_dict)"
   ],
   "id": "cdc2b069c445ba79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1089#134686': (276.21006250000005, 0, 38), '1089#134691': (206.8500625, 0, 26), '1188#133604': (491.93493750000016, 0, 45), '121#121726': (79.09, 0, 15), '121#123852': (76.64500000000001, 0, 5), '121#123859': (93.15499999999999, 0, 5), '121#127105': (231.69499999999996, 0, 37), '1221#135766': (176.6, 0, 16), '1221#135767': (307.745, 0, 25), '1284#1180': (227.9, 0, 33), '1284#1181': (146.97999999999996, 0, 22), '1284#134647': (114.5550625, 0, 8), '1320#122612': (129.125, 0, 17), '1320#122617': (352.1499375000001, 0, 42), '1580#141083': (257.0300625, 0, 54), '1580#141084': (227.25000000000003, 0, 51), '1995#1826': (164.43506250000002, 0, 27), '1995#1836': (142.08499999999998, 0, 15), '1995#1837': (177.23506250000003, 0, 30), '2094#142345': (485.64499999999975, 0, 61), '2300#131720': (491.195, 0, 42), '237#126133': (166.965, 0, 26), '237#134493': (115.01500000000001, 0, 19), '237#134500': (199.50493749999998, 0, 43), '260#123286': (173.63, 0, 32), '260#123288': (204.04506250000006, 0, 29), '260#123440': (105.43999999999998, 0, 21), '2830#3979': (92.1450625, 0, 13), '2830#3980': (390.30506249999996, 0, 77), '2961#960': (281.9850625, 0, 23), '2961#961': (202.08999999999997, 0, 23), '3570#5694': (223.76500000000001, 0, 23), '3570#5695': (143.245, 0, 16), '3570#5696': (115.85, 0, 11), '3575#170457': (483.6251874999999, 0, 57), '3729#6852': (481.68999999999994, 0, 47), '4077#13751': (273.26000000000005, 0, 22), '4077#13754': (215.21493750000005, 0, 17), '4446#2271': (123.715, 0, 25), '4446#2273': (171.445, 0, 37), '4446#2275': (184.99000000000004, 0, 46), '4507#16021': (483.295, 0, 60), '4970#29093': (210.885, 0, 24), '4970#29095': (278.30999999999995, 0, 39), '4992#23283': (144.42499999999998, 0, 21), '4992#41797': (175.83499999999998, 0, 23), '4992#41806': (172.08, 0, 18), '5105#28233': (118.78500000000001, 0, 11), '5105#28240': (178.54506250000003, 0, 25), '5105#28241': (190.12999999999997, 0, 20), '5142#33396': (264.015, 0, 69), '5142#36377': (180.70000000000002, 0, 26), '5142#36586': (16.82, 0, 5), '5142#36600': (22.709999999999997, 0, 2), '5639#40744': (496.68993750000004, 0, 42), '5683#32865': (110.53999999999999, 0, 18), '5683#32866': (186.905, 0, 31), '5683#32879': (183.075, 0, 26), '61#70968': (280.99493750000005, 0, 63), '61#70970': (203.58999999999997, 0, 41), '672#122797': (496.1649999999999, 0, 75), '6829#68769': (247.80499999999998, 0, 54), '6829#68771': (246.655, 0, 37), '6930#75918': (178.88506249999998, 0, 21), '6930#76324': (149.38000000000002, 0, 29), '6930#81414': (151.785, 0, 28), '7021#79730': (123.6, 0, 10), '7021#79740': (122.05000000000001, 0, 15), '7021#79759': (54.614999999999995, 0, 6), '7021#85628': (188.22500000000002, 0, 28), '7127#75946': (235.74, 0, 30), '7127#75947': (263.26493750000003, 0, 41), '7176#88083': (203.575, 0, 28), '7176#92135': (279.87000000000006, 0, 46), '7729#102255': (489.94506250000006, 0, 47), '8224#274381': (331.1649999999999, 0, 18), '8224#274384': (163.4, 0, 14), '8230#279154': (494.80000000000007, 0, 44), '8455#210777': (481.8549999999999, 0, 71), '8463#287645': (113.235, 0, 15), '8463#294825': (147.72499999999997, 0, 20), '8463#294828': (222.06, 0, 39), '8555#284447': (190.46506250000007, 0, 25), '8555#284449': (160.62500000000003, 0, 21), '8555#292519': (130.99499999999998, 0, 16), '908#157963': (266.47999999999996, 0, 31), '908#31957': (216.35, 0, 26)}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:33:17.825897Z",
     "start_time": "2025-07-09T06:33:17.813182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sort by total duration\n",
    "speakerchapter_dict = dict(sorted(speakerchapter_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "speakerchapter_dict"
   ],
   "id": "e322a621f4937968",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5639#40744': (496.68993750000004, 0, 42),\n",
       " '672#122797': (496.1649999999999, 0, 75),\n",
       " '8230#279154': (494.80000000000007, 0, 44),\n",
       " '1188#133604': (491.93493750000016, 0, 45),\n",
       " '2300#131720': (491.195, 0, 42),\n",
       " '7729#102255': (489.94506250000006, 0, 47),\n",
       " '2094#142345': (485.64499999999975, 0, 61),\n",
       " '3575#170457': (483.6251874999999, 0, 57),\n",
       " '4507#16021': (483.295, 0, 60),\n",
       " '8455#210777': (481.8549999999999, 0, 71),\n",
       " '3729#6852': (481.68999999999994, 0, 47),\n",
       " '2830#3980': (390.30506249999996, 0, 77),\n",
       " '1320#122617': (352.1499375000001, 0, 42),\n",
       " '8224#274381': (331.1649999999999, 0, 18),\n",
       " '1221#135767': (307.745, 0, 25),\n",
       " '2961#960': (281.9850625, 0, 23),\n",
       " '61#70968': (280.99493750000005, 0, 63),\n",
       " '7176#92135': (279.87000000000006, 0, 46),\n",
       " '4970#29095': (278.30999999999995, 0, 39),\n",
       " '1089#134686': (276.21006250000005, 0, 38),\n",
       " '4077#13751': (273.26000000000005, 0, 22),\n",
       " '908#157963': (266.47999999999996, 0, 31),\n",
       " '5142#33396': (264.015, 0, 69),\n",
       " '7127#75947': (263.26493750000003, 0, 41),\n",
       " '1580#141083': (257.0300625, 0, 54),\n",
       " '6829#68769': (247.80499999999998, 0, 54),\n",
       " '6829#68771': (246.655, 0, 37),\n",
       " '7127#75946': (235.74, 0, 30),\n",
       " '121#127105': (231.69499999999996, 0, 37),\n",
       " '1284#1180': (227.9, 0, 33),\n",
       " '1580#141084': (227.25000000000003, 0, 51),\n",
       " '3570#5694': (223.76500000000001, 0, 23),\n",
       " '8463#294828': (222.06, 0, 39),\n",
       " '908#31957': (216.35, 0, 26),\n",
       " '4077#13754': (215.21493750000005, 0, 17),\n",
       " '4970#29093': (210.885, 0, 24),\n",
       " '1089#134691': (206.8500625, 0, 26),\n",
       " '260#123288': (204.04506250000006, 0, 29),\n",
       " '61#70970': (203.58999999999997, 0, 41),\n",
       " '7176#88083': (203.575, 0, 28),\n",
       " '2961#961': (202.08999999999997, 0, 23),\n",
       " '237#134500': (199.50493749999998, 0, 43),\n",
       " '8555#284447': (190.46506250000007, 0, 25),\n",
       " '5105#28241': (190.12999999999997, 0, 20),\n",
       " '7021#85628': (188.22500000000002, 0, 28),\n",
       " '5683#32866': (186.905, 0, 31),\n",
       " '4446#2275': (184.99000000000004, 0, 46),\n",
       " '5683#32879': (183.075, 0, 26),\n",
       " '5142#36377': (180.70000000000002, 0, 26),\n",
       " '6930#75918': (178.88506249999998, 0, 21),\n",
       " '5105#28240': (178.54506250000003, 0, 25),\n",
       " '1995#1837': (177.23506250000003, 0, 30),\n",
       " '1221#135766': (176.6, 0, 16),\n",
       " '4992#41797': (175.83499999999998, 0, 23),\n",
       " '260#123286': (173.63, 0, 32),\n",
       " '4992#41806': (172.08, 0, 18),\n",
       " '4446#2273': (171.445, 0, 37),\n",
       " '237#126133': (166.965, 0, 26),\n",
       " '1995#1826': (164.43506250000002, 0, 27),\n",
       " '8224#274384': (163.4, 0, 14),\n",
       " '8555#284449': (160.62500000000003, 0, 21),\n",
       " '6930#81414': (151.785, 0, 28),\n",
       " '6930#76324': (149.38000000000002, 0, 29),\n",
       " '8463#294825': (147.72499999999997, 0, 20),\n",
       " '1284#1181': (146.97999999999996, 0, 22),\n",
       " '4992#23283': (144.42499999999998, 0, 21),\n",
       " '3570#5695': (143.245, 0, 16),\n",
       " '1995#1836': (142.08499999999998, 0, 15),\n",
       " '8555#292519': (130.99499999999998, 0, 16),\n",
       " '1320#122612': (129.125, 0, 17),\n",
       " '4446#2271': (123.715, 0, 25),\n",
       " '7021#79730': (123.6, 0, 10),\n",
       " '7021#79740': (122.05000000000001, 0, 15),\n",
       " '5105#28233': (118.78500000000001, 0, 11),\n",
       " '3570#5696': (115.85, 0, 11),\n",
       " '237#134493': (115.01500000000001, 0, 19),\n",
       " '1284#134647': (114.5550625, 0, 8),\n",
       " '8463#287645': (113.235, 0, 15),\n",
       " '5683#32865': (110.53999999999999, 0, 18),\n",
       " '260#123440': (105.43999999999998, 0, 21),\n",
       " '121#123859': (93.15499999999999, 0, 5),\n",
       " '2830#3979': (92.1450625, 0, 13),\n",
       " '121#121726': (79.09, 0, 15),\n",
       " '121#123852': (76.64500000000001, 0, 5),\n",
       " '7021#79759': (54.614999999999995, 0, 6),\n",
       " '5142#36600': (22.709999999999997, 0, 2),\n",
       " '5142#36586': (16.82, 0, 5)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:33:19.840423Z",
     "start_time": "2025-07-09T06:33:19.837459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "heap = [(-dur, key) for key, (dur, _, _) in speakerchapter_dict.items()]\n",
    "heapq.heapify(heap)\n",
    "long_entities = []"
   ],
   "id": "7739dd0fba9fdffc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:33:25.311239Z",
     "start_time": "2025-07-09T06:33:25.305987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_audio_file_name(key, used_seg):\n",
    "    speaker = key.split('#')[0]\n",
    "    chapter = key.split('#')[1]\n",
    "    return os.path.join(speaker, chapter, f\"{speaker}-{chapter}-{used_seg:04d}.flac\")\n",
    "\n",
    "def get_transcribe(key, used_seg):\n",
    "    speaker = key.split('#')[0]\n",
    "    chapter = key.split('#')[1]\n",
    "    fileindex = f\"{speaker}-{chapter}-{used_seg:04d}\"\n",
    "    transcribe_path = os.path.join(INPUTDIR, speaker, chapter, f\"{speaker}-{chapter}.trans.txt\")\n",
    "    with open(transcribe_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(fileindex):\n",
    "                return line.strip().split(' ', 1)[1]\n",
    "    return None\n",
    "\n",
    "\n"
   ],
   "id": "14c575de0ff22943",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:33:27.727869Z",
     "start_time": "2025-07-09T06:33:27.724621Z"
    }
   },
   "cell_type": "code",
   "source": "MAX_PARTS = 5",
   "id": "a2bbe434658fdf36",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T06:48:53.124757Z",
     "start_time": "2025-07-09T06:48:47.860842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counter = count(start=config['amount'])\n",
    "with open(os.path.join(OUT_DIR, OUT_FILE_NAME), \"a\") as output_f:\n",
    "    while heap:                                  # ❶ 堆里还有章节\n",
    "        ent = LibriSpeechEntity(next(counter))   # 先建一条长音频\n",
    "        print(f\"[NEW] {ent.id}\")\n",
    "        # ❷ 不断取“剩余最长”的 chapter 给同一条 entity 填料\n",
    "        while len(ent.components) < MAX_PARTS and heap:\n",
    "            neg_dur, key = heapq.heappop(heap)\n",
    "            remain_dur, used_seg, total_seg = speakerchapter_dict[key]\n",
    "            speaker, chapter = key.split('#')\n",
    "\n",
    "            while used_seg <= total_seg and len(ent.components) < MAX_PARTS:\n",
    "                filename   = f\"{speaker}-{chapter}-{used_seg:04d}.flac\"\n",
    "                audio_path = os.path.join(INPUTDIR, speaker, chapter, filename)\n",
    "                audio, src = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
    "                text       = get_transcribe(key, used_seg)\n",
    "                print(f\"[ADD] {audio_path}  {text}\")\n",
    "\n",
    "                if not ent.appendaudio(audio, text, filename):\n",
    "                    break\n",
    "\n",
    "                seg_len_sec  = len(audio) / SAMPLE_RATE\n",
    "                remain_dur  -= seg_len_sec\n",
    "                used_seg    += 1\n",
    "                print(f\"[ADDED] {used_seg}\")\n",
    "\n",
    "            # ----- 废物利用 -----\n",
    "            speakerchapter_dict[key] = (remain_dur, used_seg, total_seg)\n",
    "            if used_seg <= total_seg:                     # 章里还有料\n",
    "                heapq.heappush(heap, (-remain_dur, key))\n",
    "\n",
    "\n",
    "        if ent.finished:\n",
    "            wav_path = ent.export_wav()\n",
    "            output_f.write(ent.get_metadata() + \"\\n\")\n",
    "            print(f\"[OK] {wav_path}  {ent.duration_sec:.1f}s  {len(ent.components)} seg\")\n",
    "        else:\n",
    "            break"
   ],
   "id": "4d86e3bbc185ff10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW] 4\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n",
      "[ADD] /mnt/d/voicedata/test-clean/LibriSpeech/test-clean/2300/131720/2300-131720-0000.flac  THE PARIS PLANT LIKE THAT AT THE CRYSTAL PALACE WAS A TEMPORARY EXHIBIT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m text       \u001B[38;5;241m=\u001B[39m get_transcribe(key, used_seg)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[ADD] \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maudio_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43ment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappendaudio\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     21\u001B[0m seg_len_sec  \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(audio) \u001B[38;5;241m/\u001B[39m SAMPLE_RATE\n",
      "File \u001B[0;32m/mnt/d/repo/LongSpeech/LibriSpeechEntity.py:25\u001B[0m, in \u001B[0;36mLibriSpeechEntity.appendaudio\u001B[0;34m(self, audio_data, transcribe, path)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mappendaudio\u001B[39m(\u001B[38;5;28mself\u001B[39m, audio_data, transcribe, path):\n\u001B[0;32m---> 25\u001B[0m     transcribe \u001B[38;5;241m=\u001B[39m \u001B[43mrestore_punctuation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtranscribe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mappendaudio(audio_data, transcribe, path)\n",
      "File \u001B[0;32m/mnt/d/repo/LongSpeech/util.py:10\u001B[0m, in \u001B[0;36mrestore_punctuation\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrestore_punctuation\u001B[39m(text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m      5\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m    标点恢复\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m     punctuated_text \u001B[38;5;241m=\u001B[39m \u001B[43mpunct\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrestore_punctuation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# capitailize the first letter of each sentence\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m punctuated_text\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/deepmultilingualpunctuation/punctuationmodel.py:21\u001B[0m, in \u001B[0;36mPunctuationModel.restore_punctuation\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrestore_punctuation\u001B[39m(\u001B[38;5;28mself\u001B[39m,text):        \n\u001B[0;32m---> 21\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_to_text(result)\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/deepmultilingualpunctuation/punctuationmodel.py:48\u001B[0m, in \u001B[0;36mPunctuationModel.predict\u001B[0;34m(self, words)\u001B[0m\n\u001B[1;32m     46\u001B[0m     overlap \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     47\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(batch)\n\u001B[0;32m---> 48\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m      \n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) \u001B[38;5;241m==\u001B[39m result[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunk size too large, text got clipped\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     51\u001B[0m char_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:250\u001B[0m, in \u001B[0;36mTokenClassificationPipeline.__call__\u001B[0;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m offset_mapping:\n\u001B[1;32m    248\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffset_mapping\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m offset_mapping\n\u001B[0;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/base.py:1294\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[0;32m-> 1294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1295\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1296\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_iterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1297\u001B[0m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\n\u001B[1;32m   1298\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1299\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1300\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1301\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1302\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:269\u001B[0m, in \u001B[0;36mPipelinePackIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    266\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m accumulator\n\u001B[1;32m    268\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_last:\n\u001B[0;32m--> 269\u001B[0m     processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    271\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed, torch\u001B[38;5;241m.\u001B[39mTensor):\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/base.py:1209\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1207\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1208\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1209\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1210\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:287\u001B[0m, in \u001B[0;36mTokenClassificationPipeline._forward\u001B[0;34m(self, model_inputs)\u001B[0m\n\u001B[1;32m    285\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 287\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    288\u001B[0m     logits \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m output[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m: logits,\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspecial_tokens_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: special_tokens_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs,\n\u001B[1;32m    297\u001B[0m }\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1523\u001B[0m, in \u001B[0;36mXLMRobertaForTokenClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1518\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1519\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001B[39;00m\n\u001B[1;32m   1520\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1521\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1523\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1535\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1537\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sequence_output)\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:977\u001B[0m, in \u001B[0;36mXLMRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    970\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[1;32m    971\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[1;32m    972\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[1;32m    974\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[1;32m    975\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m--> 977\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    986\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    987\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    990\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:632\u001B[0m, in \u001B[0;36mXLMRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    621\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    622\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    623\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    629\u001B[0m         output_attentions,\n\u001B[1;32m    630\u001B[0m     )\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 632\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    640\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    642\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    643\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:521\u001B[0m, in \u001B[0;36mXLMRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    511\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    518\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    520\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 521\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    528\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    530\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:448\u001B[0m, in \u001B[0;36mXLMRobertaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    440\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    446\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    447\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 448\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    449\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    450\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    457\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    458\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:218\u001B[0m, in \u001B[0;36mXLMRobertaSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    217\u001B[0m     key_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey(hidden_states))\n\u001B[0;32m--> 218\u001B[0m     value_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    220\u001B[0m query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(mixed_query_layer)\n\u001B[1;32m    222\u001B[0m use_cache \u001B[38;5;241m=\u001B[39m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
