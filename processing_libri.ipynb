{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-08T05:06:22.361040Z",
     "start_time": "2025-07-08T05:06:20.989373Z"
    }
   },
   "source": [
    "from datasets import load_dataset, Audio, IterableDataset\n",
    "from itertools import chain\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T05:42:19.582088Z",
     "start_time": "2025-07-08T05:42:19.577601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_DURATION = 600\n",
    "SAMPLE_RATE = 16000"
   ],
   "id": "4b7ac57e926314a9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T05:06:29.462251Z",
     "start_time": "2025-07-08T05:06:23.507564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "ds_dict = load_dataset(\n",
    "        \"openslr/librispeech_asr\",\n",
    "        \"all\",\n",
    "        streaming=True,\n",
    "    )"
   ],
   "id": "5c5a1f7a3f9c15fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renyi/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/datasets/load.py:1461: FutureWarning: The repository for openslr/librispeech_asr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/openslr/librispeech_asr\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T05:40:58.143979Z",
     "start_time": "2025-07-08T05:40:58.138344Z"
    }
   },
   "cell_type": "code",
   "source": "print(ds_dict)",
   "id": "c4cd338b6f4d8090",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDatasetDict({\n",
      "    train.clean.100: IterableDataset({\n",
      "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "        n_shards: 1\n",
      "    })\n",
      "    train.clean.360: IterableDataset({\n",
      "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "        n_shards: 1\n",
      "    })\n",
      "    train.other.500: IterableDataset({\n",
      "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "        n_shards: 1\n",
      "    })\n",
      "    validation.clean: IterableDataset({\n",
      "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "        n_shards: 1\n",
      "    })\n",
      "    validation.other: IterableDataset({\n",
      "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "        n_shards: 1\n",
      "    })\n",
      "    test.clean: IterableDataset({\n",
      "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "        n_shards: 1\n",
      "    })\n",
      "    test.other: IterableDataset({\n",
      "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
      "        n_shards: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "85a549c031318ee9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T06:15:13.880621Z",
     "start_time": "2025-07-08T06:13:56.105690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for split_name, split_dataset in ds_dict.items():\n",
    "    split_dataset = split_dataset.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n",
    "    print(f\"Split: {split_name}\")\n",
    "    i = 0\n",
    "    for sample in split_dataset:\n",
    "        print(f\"Speaker ID: {sample['speaker_id']}, Chapter ID: {sample['chapter_id']}, file: {sample['file']}\" )\n",
    "        i += 1\n",
    "        if i >= 5:  # Limit to first 5 samples for brevity\n",
    "            break\n"
   ],
   "id": "398815417917070d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: train.clean.100\n",
      "Speaker ID: 374, Chapter ID: 180298, file: 374-180298-0000.flac\n",
      "Speaker ID: 374, Chapter ID: 180298, file: 374-180298-0001.flac\n",
      "Speaker ID: 374, Chapter ID: 180298, file: 374-180298-0002.flac\n",
      "Speaker ID: 374, Chapter ID: 180298, file: 374-180298-0003.flac\n",
      "Speaker ID: 374, Chapter ID: 180298, file: 374-180298-0004.flac\n",
      "Split: train.clean.360\n",
      "Speaker ID: 1487, Chapter ID: 133273, file: 1487-133273-0000.flac\n",
      "Speaker ID: 1487, Chapter ID: 133273, file: 1487-133273-0001.flac\n",
      "Speaker ID: 1487, Chapter ID: 133273, file: 1487-133273-0002.flac\n",
      "Speaker ID: 1487, Chapter ID: 133273, file: 1487-133273-0003.flac\n",
      "Speaker ID: 1487, Chapter ID: 133273, file: 1487-133273-0004.flac\n",
      "Split: train.other.500\n",
      "Speaker ID: 8296, Chapter ID: 266250, file: 8296-266250-0000.flac\n",
      "Speaker ID: 8296, Chapter ID: 266250, file: 8296-266250-0001.flac\n",
      "Speaker ID: 8296, Chapter ID: 266250, file: 8296-266250-0002.flac\n",
      "Speaker ID: 8296, Chapter ID: 266250, file: 8296-266250-0003.flac\n",
      "Speaker ID: 8296, Chapter ID: 266250, file: 8296-266250-0004.flac\n",
      "Split: validation.clean\n",
      "Speaker ID: 2277, Chapter ID: 149896, file: 2277-149896-0000.flac\n",
      "Speaker ID: 2277, Chapter ID: 149896, file: 2277-149896-0001.flac\n",
      "Speaker ID: 2277, Chapter ID: 149896, file: 2277-149896-0002.flac\n",
      "Speaker ID: 2277, Chapter ID: 149896, file: 2277-149896-0003.flac\n",
      "Speaker ID: 2277, Chapter ID: 149896, file: 2277-149896-0004.flac\n",
      "Split: validation.other\n",
      "Speaker ID: 3660, Chapter ID: 172183, file: 3660-172183-0000.flac\n",
      "Speaker ID: 3660, Chapter ID: 172183, file: 3660-172183-0001.flac\n",
      "Speaker ID: 3660, Chapter ID: 172183, file: 3660-172183-0002.flac\n",
      "Speaker ID: 3660, Chapter ID: 172183, file: 3660-172183-0003.flac\n",
      "Speaker ID: 3660, Chapter ID: 172183, file: 3660-172183-0004.flac\n",
      "Split: test.clean\n",
      "Speaker ID: 6930, Chapter ID: 75918, file: 6930-75918-0000.flac\n",
      "Speaker ID: 6930, Chapter ID: 75918, file: 6930-75918-0001.flac\n",
      "Speaker ID: 6930, Chapter ID: 75918, file: 6930-75918-0002.flac\n",
      "Speaker ID: 6930, Chapter ID: 75918, file: 6930-75918-0003.flac\n",
      "Speaker ID: 6930, Chapter ID: 75918, file: 6930-75918-0004.flac\n",
      "Split: test.other\n",
      "Speaker ID: 7902, Chapter ID: 96591, file: 7902-96591-0000.flac\n",
      "Speaker ID: 7902, Chapter ID: 96591, file: 7902-96591-0001.flac\n",
      "Speaker ID: 7902, Chapter ID: 96591, file: 7902-96591-0002.flac\n",
      "Speaker ID: 7902, Chapter ID: 96591, file: 7902-96591-0003.flac\n",
      "Speaker ID: 7902, Chapter ID: 96591, file: 7902-96591-0004.flac\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f5e5fd4420ed910"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T03:29:04.015132Z",
     "start_time": "2025-07-08T03:29:04.004453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stream_librispeech_grouped_all(target_len_sec=30.0, sampling_rate=16000):\n",
    "    ds_dict = load_dataset(\n",
    "        \"openslr/librispeech_asr\",\n",
    "        \"all\",\n",
    "        streaming=True,\n",
    "    )\n",
    "\n",
    "    def every_sample():\n",
    "        for split in ds_dict.values():\n",
    "            split = split.cast_column(\"audio\",\n",
    "                                       Audio(sampling_rate=sampling_rate))\n",
    "            for sample in split:\n",
    "                yield sample\n",
    "\n",
    "    buffer, buf_dur, current_key = [], 0.0, None\n",
    "    for s in every_sample():\n",
    "        key = (s[\"speaker_id\"], s[\"chapter_id\"])\n",
    "        wav = s[\"audio\"][\"array\"]\n",
    "        dur = len(wav) / sampling_rate\n",
    "\n",
    "        if current_key is None:\n",
    "            current_key = key\n",
    "\n",
    "        # 遇到新卷或时长超阈值 → 把缓冲区吐出来\n",
    "        if key != current_key or buf_dur + dur >= target_len_sec:\n",
    "            if buffer:\n",
    "                yield {\n",
    "                    \"speaker_id\": current_key[0],\n",
    "                    \"chapter_id\": current_key[1],\n",
    "                    \"audio\": np.concatenate(buffer),\n",
    "                    \"length_sec\": buf_dur,\n",
    "                }\n",
    "            buffer, buf_dur, current_key = [], 0.0, key\n",
    "\n",
    "        buffer.append(wav); buf_dur += dur\n",
    "\n",
    "    # 文件结尾还有残余\n",
    "    if buffer:\n",
    "        yield {\n",
    "            \"speaker_id\": current_key[0],\n",
    "            \"chapter_id\": current_key[1],\n",
    "            \"audio\": np.concatenate(buffer),\n",
    "            \"length_sec\": buf_dur,\n",
    "        }\n"
   ],
   "id": "3dfc5ec7c2ea33cc",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
