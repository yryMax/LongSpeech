{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:08:43.833122Z",
     "start_time": "2025-07-18T12:08:43.809415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/workspace/renyi/LongSpeech/util.py:3: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n",
      "Device set to use cuda:0\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from lhotse import CutSet\n",
    "from mylhotse.spgispeech import prepare_spgispeech\n",
    "from lhotse.cut import append_cuts\n",
    "import logging\n",
    "from util import restore_punctuation\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:53:37.754848Z",
     "start_time": "2025-07-18T09:53:37.713706Z"
    }
   },
   "outputs": [],
   "source": [
    "IN_DIR = \"../datasets/LongSpeechSource/spgispeech\"\n",
    "OUT_DIR = '/home/yangrenyi.yry/LongSpeech_p3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'recordings': RecordingSet(len=1966109),\n",
       "  'supervisions': SupervisionSet(len=1966109)},\n",
       " 'val': {'recordings': RecordingSet(len=39341),\n",
       "  'supervisions': SupervisionSet(len=39341)}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifests = prepare_spgispeech(corpus_dir=IN_DIR, output_dir=OUT_DIR, num_jobs=15)\n",
    "manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:53:40.725694Z",
     "start_time": "2025-07-18T09:53:40.716315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "config = json.load(open(os.path.join(OUT_DIR, 'metadata.json')))\n",
    "AVG_DURATION = config['avg_duration']\n",
    "SAMPLE_RATE = config['sample_rate']\n",
    "OUT_FILE_NAME = config['source']\n",
    "prev_amount = config['amount']\n",
    "print(prev_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:04:22.686113Z",
     "start_time": "2025-07-18T09:53:43.553161Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/lazy.py:683: UserWarning: A lambda was passed to LazyMapper: it may prevent you from forking this process. If you experience issues with num_workers > 0 in torch.utils.data.DataLoader, try passing a regular function instead.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "cuts = CutSet()\n",
    "for part in manifests.keys():\n",
    "    rs = manifests[part]['recordings']\n",
    "    ss = manifests[part]['supervisions']\n",
    "    ss_punc = ss.map(lambda seg: seg.transform_text(restore_punctuation))\n",
    "    cut = CutSet.from_manifests(recordings=rs, supervisions=ss_punc)\n",
    "    cuts += cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:50:35.802808Z",
     "start_time": "2025-07-18T10:50:31.566984Z"
    }
   },
   "outputs": [],
   "source": [
    "cuts.to_jsonl(OUT_DIR + \"/spgi_raw_cuts.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:09:06.565096Z",
     "start_time": "2025-07-18T12:09:06.550442Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_group(\n",
    "        df: pd.DataFrame,\n",
    "    ):\n",
    "\n",
    "    df = df[['id', 'duration']].copy()\n",
    "\n",
    "    parts = df['id'].str.split('-').str[0].str.split('_', expand=True)\n",
    "\n",
    "    df[['speaker', 'segment_num']] = parts[[0, 1]]\n",
    "    df['segment_num'] = df['segment_num'].astype(int)\n",
    "    df['duration'] = df['duration'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def group_segments(\n",
    "    df: pd.DataFrame,\n",
    "    target_sec: int = 600,\n",
    "    tol_sec: int = 90\n",
    "):\n",
    "\n",
    "    df_sorted = df.sort_values([\"speaker\", \"segment_num\"]).reset_index(drop=True)\n",
    "\n",
    "    lower, upper = target_sec - tol_sec, target_sec + tol_sec\n",
    "    groups, cur_ids, cur_dur, cur_spk = [], [], 0.0, None\n",
    "\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        spk, seg_id, dur = row[\"speaker\"], row[\"id\"], float(row[\"duration\"])\n",
    "\n",
    "        if cur_spk is not None and spk != cur_spk and cur_dur >= lower:\n",
    "            groups.append(cur_ids)\n",
    "            cur_ids, cur_dur = [], 0.0\n",
    "\n",
    "        cur_ids.append(seg_id)\n",
    "        cur_dur += dur\n",
    "        cur_spk = spk\n",
    "\n",
    "        if cur_dur >= upper:\n",
    "            groups.append(cur_ids)\n",
    "            cur_ids, cur_dur, cur_spk = [], 0.0, None\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:11:13.193431Z",
     "start_time": "2025-07-18T12:11:10.734125Z"
    }
   },
   "outputs": [],
   "source": [
    "source_df = pd.read_json(OUT_DIR + \"/spgi_raw_cuts.jsonl\", lines=True)\n",
    "processed_df = prepare_and_group(df=source_df)\n",
    "real_strategy = group_segments(df=processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:10:24.426606Z",
     "start_time": "2025-07-18T12:10:24.420383Z"
    }
   },
   "outputs": [],
   "source": [
    "def from_strategy_to_cuts(source_cuts, strategy: list, starting_cut_id=0):\n",
    "    src_cuts = {c.id: c for c in source_cuts}\n",
    "    target_cuts_list = []\n",
    "    i = starting_cut_id\n",
    "    for cluster_ids in strategy:\n",
    "        grouped_cuts = [src_cuts[cid] for cid in cluster_ids]\n",
    "        new_id = f\"{i:06d}\"\n",
    "        merged = append_cuts(grouped_cuts).with_id(new_id)\n",
    "        target_cuts_list.append(merged)\n",
    "        i += 1\n",
    "    return CutSet(target_cuts_list), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:06.737345Z",
     "start_time": "2025-07-18T12:13:02.835916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28317"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_cuts, new_amount = from_strategy_to_cuts(cuts, real_strategy, starting_cut_id=prev_amount)\n",
    "grouped_cuts.to_jsonl(OUT_DIR + \"/spgi_grouped_cuts.jsonl\")\n",
    "new_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:12:16.177382Z",
     "start_time": "2025-07-18T12:12:16.172771Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_record(source_jsonl_path: str, target_jsonl_path: str, map_fn):\n",
    "    with open(source_jsonl_path, \"r\", encoding=\"utf-8\") as src_f, \\\n",
    "         open(target_jsonl_path, \"a\", encoding=\"utf-8\") as tgt_f:\n",
    "        for line in src_f:\n",
    "            item = json.loads(line)\n",
    "            new_item = map_fn(item)\n",
    "            tgt_f.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:12:46.392117Z",
     "start_time": "2025-07-18T12:12:46.384430Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_from_spgi_to_allaudios(one_cut, lang = \"en\"):\n",
    "    \"\"\"\n",
    "    Convert a single Commonvoice json record to a list of LongSpeech metadata.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    speakers = set()\n",
    "    total_dur = 0\n",
    "    transcripts = []\n",
    "    slices = []\n",
    "\n",
    "    for subcut in one_cut[\"tracks\"]:\n",
    "        total_dur += subcut[\"cut\"][\"duration\"]\n",
    "        full_pth = subcut[\"cut\"][\"recording\"][\"sources\"][0][\"source\"]\n",
    "        slices.append([subcut[\"cut\"][\"start\"], subcut[\"cut\"][\"duration\"]])\n",
    "        sources.append(full_pth.split(\"spgispeech\")[-1])\n",
    "        [speakers.add(s[\"speaker\"]) for s in subcut[\"cut\"][\"supervisions\"] if s[\"speaker\"]]\n",
    "        transcript_param = \" \".join([s[\"text\"] for s in subcut[\"cut\"][\"supervisions\"] if s[\"text\"]])\n",
    "        if transcript_param != \"\":\n",
    "            transcripts.append(transcript_param)\n",
    "        else:\n",
    "            print(subcut)\n",
    "\n",
    "    return {\n",
    "        \"id\": one_cut[\"id\"],\n",
    "        \"source_ds\": \"spgispeech\",\n",
    "        \"duration_sec\": total_dur,\n",
    "        \"audio_auto\": False,\n",
    "        \"text_auto\": False,\n",
    "        \"language\": lang,\n",
    "        \"num_speakers\": len(speakers),\n",
    "        \"num_switches\": len(speakers),\n",
    "        \"slice\": slices,\n",
    "        \"transcribe\": \" \".join(transcripts),\n",
    "        \"components\": sources,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:13.199138Z",
     "start_time": "2025-07-18T12:13:10.434665Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tracks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m OUT_DIR \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/yangrenyi.yry/LongSpeech_p3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m convert_record(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(OUT_DIR, \u001b[39m\"\u001b[39;49m\u001b[39mspgi_grouped_cuts.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      3\u001b[0m                os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(OUT_DIR, OUT_FILE_NAME),\n\u001b[1;32m      4\u001b[0m                json_from_spgi_to_allaudios)\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mconvert_record\u001b[0;34m(source_jsonl_path, target_jsonl_path, map_fn)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m src_f:\n\u001b[1;32m      5\u001b[0m     item \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(line)\n\u001b[0;32m----> 6\u001b[0m     new_item \u001b[39m=\u001b[39m map_fn(item)\n\u001b[1;32m      7\u001b[0m     tgt_f\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39mdumps(new_item, ensure_ascii\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mjson_from_spgi_to_allaudios\u001b[0;34m(one_cut, lang)\u001b[0m\n\u001b[1;32m      8\u001b[0m transcripts \u001b[39m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m slices \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m subcut \u001b[39min\u001b[39;00m one_cut[\u001b[39m\"\u001b[39;49m\u001b[39mtracks\u001b[39;49m\u001b[39m\"\u001b[39;49m]:\n\u001b[1;32m     12\u001b[0m     total_dur \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m subcut[\u001b[39m\"\u001b[39m\u001b[39mcut\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m     full_pth \u001b[39m=\u001b[39m subcut[\u001b[39m\"\u001b[39m\u001b[39mcut\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrecording\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msources\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tracks'"
     ]
    }
   ],
   "source": [
    "OUT_DIR = '/home/yangrenyi.yry/LongSpeech_p3'\n",
    "convert_record(os.path.join(OUT_DIR, \"spgi_grouped_cuts.jsonl\"),\n",
    "               os.path.join(OUT_DIR, OUT_FILE_NAME),\n",
    "               json_from_spgi_to_allaudios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:16.091137Z",
     "start_time": "2025-07-18T12:13:16.087667Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "from worker import save_one_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:19.787598Z",
     "start_time": "2025-07-18T12:13:19.778135Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_audios_from_cutset(cutset, out_dir, num_jobs=None):\n",
    "    if num_jobs is None:\n",
    "        num_jobs = os.cpu_count()\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cuts_to_process = [\n",
    "        cut for cut in tqdm(cutset, desc=\"Checking for existing files\") \n",
    "        if not (out_dir / f\"{cut.id}.wav\").exists()\n",
    "    ]\n",
    "    context = mp.get_context(\"spawn\")\n",
    "    with ProcessPoolExecutor(max_workers=num_jobs, mp_context=context) as pool:\n",
    "        futures = [\n",
    "            pool.submit(save_one_worker, cut, out_dir)\n",
    "            for cut in tqdm(cuts_to_process, desc=\"1. 提交任务中\")\n",
    "        ]\n",
    "        for _ in tqdm(\n",
    "        as_completed(futures),\n",
    "        total=len(futures),\n",
    "        desc=f\"Saving WAVs ({num_jobs} workers)\"\n",
    "        ):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28316"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_cuts = CutSet.from_jsonl(os.path.join(OUT_DIR, 'spgi_grouped_cuts.jsonl'))\n",
    "len(grouped_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:18:47.614102Z",
     "start_time": "2025-07-18T12:13:24.266869Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking for existing files: 100%|██████████| 28316/28316 [01:13<00:00, 383.32it/s]\n",
      "1. 提交任务中: 100%|██████████| 14890/14890 [00:00<00:00, 35788.87it/s]\n",
      "Saving WAVs (15 workers):   3%|▎         | 400/14890 [01:26<1:04:12,  3.76it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "save_audios_from_cutset(grouped_cuts, os.path.join(OUT_DIR, 'wavs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['amount'] = prev_amount + len(grouped_cuts) \n",
    "with open(os.path.join(OUT_DIR, 'metadata.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(config, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
