{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:08:43.833122Z",
     "start_time": "2025-07-18T12:08:43.809415Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from lhotse import CutSet\n",
    "from mylhotse.spgispeech import prepare_spgispeech\n",
    "from lhotse.cut import append_cuts\n",
    "import logging\n",
    "from util import restore_punctuation\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "logging.basicConfig(level=logging.INFO)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:53:37.754848Z",
     "start_time": "2025-07-18T09:53:37.713706Z"
    }
   },
   "source": [
    "IN_DIR = \"../datasets/LongSpeechSource/TEDLIUM_release-3\"\n",
    "IN_DIR = \"/mnt/d/voicedata/spgispeech/spgispeech\"\n",
    "OUT_DIR = '../datasets/LongSpeech'\n",
    "manifests = prepare_spgispeech(corpus_dir=IN_DIR, output_dir=OUT_DIR, num_jobs=15)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing SPGISpeech subset: val\n",
      "INFO:root:SPGISpeech subset: val already prepared - skipping.\n",
      "WARNING:root:Manifests are lazily materialized. You may want to call `lhotse.qa.fix_manifests()` to ensure that all supervisions fall within the corresponding recordings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_dir /mnt/d/voicedata/spgispeech/spgispeech\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T09:53:40.725694Z",
     "start_time": "2025-07-18T09:53:40.716315Z"
    }
   },
   "source": [
    "config = json.load(open(os.path.join(OUT_DIR, 'metadata.json')))\n",
    "AVG_DURATION = config['avg_duration']\n",
    "SAMPLE_RATE = config['sample_rate']\n",
    "OUT_FILE_NAME = config['source']\n",
    "prev_amount = config['amount']\n",
    "print(prev_amount)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:04:22.686113Z",
     "start_time": "2025-07-18T09:53:43.553161Z"
    }
   },
   "source": [
    "cuts = CutSet()\n",
    "for part in manifests.keys():\n",
    "    rs = manifests[part]['recordings']\n",
    "    ss = manifests[part]['supervisions']\n",
    "    ss_punc = ss.map(lambda seg: seg.transform_text(restore_punctuation))\n",
    "    cut = CutSet.from_manifests(recordings=rs, supervisions=ss_punc)\n",
    "    cuts += cut"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renyi/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/lhotse/lazy.py:683: UserWarning: A lambda was passed to LazyMapper: it may prevent you from forking this process. If you experience issues with num_workers > 0 in torch.utils.data.DataLoader, try passing a regular function instead.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:50:35.802808Z",
     "start_time": "2025-07-18T10:50:31.566984Z"
    }
   },
   "source": "cuts.to_jsonl(OUT_DIR + \"/raw_cuts.jsonl\")",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:09:06.565096Z",
     "start_time": "2025-07-18T12:09:06.550442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_and_group(\n",
    "        df: pd.DataFrame,\n",
    "    ):\n",
    "\n",
    "    df = df[['id', 'duration']].copy()\n",
    "\n",
    "    parts = df['id'].str.split('-').str[0].str.split('_', expand=True)\n",
    "\n",
    "    df[['speaker', 'segment_num']] = parts[[0, 1]]\n",
    "    df['segment_num'] = df['segment_num'].astype(int)\n",
    "    df['duration'] = df['duration'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def group_segments(\n",
    "    df: pd.DataFrame,\n",
    "    target_sec: int = 600,\n",
    "    tol_sec: int = 90\n",
    "):\n",
    "\n",
    "    df_sorted = df.sort_values([\"speaker\", \"segment_num\"]).reset_index(drop=True)\n",
    "\n",
    "    lower, upper = target_sec - tol_sec, target_sec + tol_sec\n",
    "    groups, cur_ids, cur_dur, cur_spk = [], [], 0.0, None\n",
    "\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        spk, seg_id, dur = row[\"speaker\"], row[\"id\"], float(row[\"duration\"])\n",
    "\n",
    "        if cur_spk is not None and spk != cur_spk and cur_dur >= lower:\n",
    "            groups.append(cur_ids)\n",
    "            cur_ids, cur_dur = [], 0.0\n",
    "\n",
    "        cur_ids.append(seg_id)\n",
    "        cur_dur += dur\n",
    "        cur_spk = spk\n",
    "\n",
    "        if cur_dur >= upper:\n",
    "            groups.append(cur_ids)\n",
    "            cur_ids, cur_dur, cur_spk = [], 0.0, None\n",
    "\n",
    "    if cur_ids:\n",
    "        groups.append(cur_ids)\n",
    "\n",
    "    return groups"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:11:13.193431Z",
     "start_time": "2025-07-18T12:11:10.734125Z"
    }
   },
   "source": [
    "source_df = pd.read_json(OUT_DIR + \"/spgi_raw_cuts.jsonl\", lines=True)\n",
    "processed_df = prepare_and_group(df=source_df)\n",
    "real_strategy = group_segments(df=processed_df)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:10:24.426606Z",
     "start_time": "2025-07-18T12:10:24.420383Z"
    }
   },
   "source": [
    "def from_strategy_to_cuts(source_cuts, strategy: list, starting_cut_id=0):\n",
    "    src_cuts = {c.id: c for c in source_cuts}\n",
    "    target_cuts_list = []\n",
    "    i = starting_cut_id\n",
    "    for cluster_ids in strategy:\n",
    "        grouped_cuts = [src_cuts[cid] for cid in cluster_ids]\n",
    "        new_id = f\"{i:06d}\"\n",
    "        merged = append_cuts(grouped_cuts).with_id(new_id)\n",
    "        target_cuts_list.append(merged)\n",
    "        i += 1\n",
    "    return CutSet(target_cuts_list), i"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:06.737345Z",
     "start_time": "2025-07-18T12:13:02.835916Z"
    }
   },
   "source": [
    "grouped_cuts, new_amount = from_strategy_to_cuts(cuts, real_strategy, starting_cut_id=prev_amount)\n",
    "grouped_cuts.to_jsonl(OUT_DIR + \"/spgi_grouped_cuts.jsonl\")"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:12:16.177382Z",
     "start_time": "2025-07-18T12:12:16.172771Z"
    }
   },
   "source": [
    "def convert_record(source_jsonl_path: str, target_jsonl_path: str, map_fn):\n",
    "    with open(source_jsonl_path, \"r\", encoding=\"utf-8\") as src_f, \\\n",
    "         open(target_jsonl_path, \"a\", encoding=\"utf-8\") as tgt_f:\n",
    "        for line in src_f:\n",
    "            item = json.loads(line)\n",
    "            new_item = map_fn(item)\n",
    "            tgt_f.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:12:46.392117Z",
     "start_time": "2025-07-18T12:12:46.384430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def json_from_spgi_to_allaudios(one_cut, lang = \"en\"):\n",
    "    \"\"\"\n",
    "    Convert a single Commonvoice json record to a list of LongSpeech metadata.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    speakers = set()\n",
    "    total_dur = 0\n",
    "    transcripts = []\n",
    "    slices = []\n",
    "\n",
    "    for subcut in one_cut[\"tracks\"]:\n",
    "        total_dur += subcut[\"cut\"][\"duration\"]\n",
    "        full_pth = subcut[\"cut\"][\"recording\"][\"sources\"][0][\"source\"]\n",
    "        slices.append([subcut[\"cut\"][\"start\"], subcut[\"cut\"][\"duration\"]])\n",
    "        sources.append(full_pth.split(\"spgispeech\")[-1])\n",
    "        [speakers.add(s[\"speaker\"]) for s in subcut[\"cut\"][\"supervisions\"] if s[\"speaker\"]]\n",
    "        transcript_param = \" \".join([s[\"text\"] for s in subcut[\"cut\"][\"supervisions\"] if s[\"text\"]])\n",
    "        if transcript_param != \"\":\n",
    "            transcripts.append(transcript_param)\n",
    "        else:\n",
    "            print(subcut)\n",
    "\n",
    "    return {\n",
    "        \"id\": one_cut[\"id\"],\n",
    "        \"source_ds\": \"spgispeech\",\n",
    "        \"duration_sec\": total_dur,\n",
    "        \"audio_auto\": False,\n",
    "        \"text_auto\": False,\n",
    "        \"language\": lang,\n",
    "        \"num_speakers\": len(speakers),\n",
    "        \"num_switches\": len(speakers),\n",
    "        \"slice\": slices,\n",
    "        \"transcribe\": \" \".join(transcripts),\n",
    "        \"components\": sources,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:13.199138Z",
     "start_time": "2025-07-18T12:13:10.434665Z"
    }
   },
   "source": [
    "convert_record(os.path.join(OUT_DIR, \"spgi_grouped_cuts.jsonl\"),\n",
    "               os.path.join(OUT_DIR, OUT_FILE_NAME),\n",
    "               json_from_spgi_to_allaudios)"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:16.091137Z",
     "start_time": "2025-07-18T12:13:16.087667Z"
    }
   },
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "from worker import save_one_worker"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:13:19.787598Z",
     "start_time": "2025-07-18T12:13:19.778135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_audios_from_cutset(cutset, out_dir, num_jobs=None):\n",
    "    if num_jobs is None:\n",
    "        num_jobs = os.cpu_count()\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    context = mp.get_context(\"spawn\")\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_jobs, mp_context=context) as pool:\n",
    "        futures = [\n",
    "            pool.submit(save_one_worker, cut, out_dir)\n",
    "            for cut in tqdm(cutset, desc=\"1. 提交任务中\")\n",
    "        ]\n",
    "        for _ in tqdm(\n",
    "        as_completed(futures),\n",
    "        total=len(futures),\n",
    "        desc=f\"Saving WAVs ({num_jobs} workers)\"\n",
    "        ):\n",
    "            pass"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T12:18:47.614102Z",
     "start_time": "2025-07-18T12:13:24.266869Z"
    }
   },
   "source": [
    "mp.set_start_method('spawn', force=True)\n",
    "save_audios_from_cutset(grouped_cuts, os.path.join(OUT_DIR, 'wavs'))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1. 提交任务中: 100%|██████████| 560/560 [00:01<00:00, 509.76it/s]\n",
      "Saving WAVs (24 workers): 100%|██████████| 560/560 [05:20<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
