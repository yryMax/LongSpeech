{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:39:58.339495Z",
     "start_time": "2025-07-17T03:39:58.321877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/workspace/renyi/LongSpeech/util.py:3: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n",
      "Device set to use cuda:0\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lhotse import CutSet, RecordingSet, SupervisionSet, MonoCut, combine\n",
    "from lhotse.recipes import prepare_commonvoice\n",
    "import logging\n",
    "from util import *\n",
    "import numpy as np\n",
    "import faiss, gc\n",
    "from lhotse_util import from_strategy_to_cuts, SpeakerEmbeddingExtractor\n",
    "import librosa\n",
    "from bitarray import bitarray\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a1954a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:40:03.444594Z",
     "start_time": "2025-07-17T03:40:03.441945Z"
    }
   },
   "outputs": [],
   "source": [
    "# directory paths to save audio and transcript files\n",
    "IN_DIR = \"../datasets/LongSpeechSource/cv-corpus-22.0-2025-06-20\"\n",
    "# directory paths to save metadata and processed aduio files\n",
    "OUT_DIR = '../datasets/LongSpeech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config_setup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:40:05.191837Z",
     "start_time": "2025-07-17T03:40:05.183396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31908\n"
     ]
    }
   ],
   "source": [
    "config = json.load(open(os.path.join(OUT_DIR, 'metadata.json')))\n",
    "AVG_DURATION = config['avg_duration']\n",
    "SAMPLE_RATE = config['sample_rate']\n",
    "OUT_FILE_NAME = config['source']\n",
    "prev_amount = config['amount']\n",
    "print(prev_amount)\n",
    "task = \"asr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de79901c45e3eb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:33:50.552508Z",
     "start_time": "2025-07-17T03:33:44.494337Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CommonVoice languages:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Distributing tasks: 177537it [00:08, 21790.04it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing: 100%|██████████| 177537/177537 [00:55<00:00, 3207.56it/s]\n",
      "WARNING:root:Trimmed 6 supervisions exceeding the end of the recording.\n",
      "WARNING:root:RecordingSet contains recordings with different sampling rates ({32000, 44100, 48000}). Make sure that this was intended.\n",
      "Spliting: 100%|██████████| 1/1 [01:18<00:00, 78.41s/it]\n",
      "Processing CommonVoice languages: 100%|██████████| 1/1 [01:18<00:00, 78.41s/it]\n"
     ]
    }
   ],
   "source": [
    "lang = \"ja\"\n",
    "manifests = prepare_commonvoice(corpus_dir=IN_DIR, output_dir=OUT_DIR, languages = lang , splits=['validated'], num_jobs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdae1870680c1ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:33:52.634656Z",
     "start_time": "2025-07-17T03:33:52.629306Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cuts = CutSet()\n",
    "for part in manifests.keys():\n",
    "    rs = manifests[part]['validated']['recordings']\n",
    "    ss = manifests[part]['validated']['supervisions']\n",
    "    cut = CutSet.from_manifests(recordings=rs, supervisions=ss)\n",
    "    cuts += cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8b3bcc268d9c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:34:24.069442Z",
     "start_time": "2025-07-17T03:34:24.025202Z"
    }
   },
   "outputs": [],
   "source": [
    "resampled_cuts = cuts.filter(lambda cut: cut.duration > 3).resample(SAMPLE_RATE).to_eager()\n",
    "resampled_cuts.to_jsonl(os.path.join(OUT_DIR, \"commonvoice_raw_cuts.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98e124a17f4c3bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:40:17.403393Z",
     "start_time": "2025-07-17T03:40:17.392645Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_feature(cuts: CutSet, batch_size: int = 100, dim: int = 384):\n",
    "    cut_list = cuts.to_eager()\n",
    "    n = len(cut_list)\n",
    "\n",
    "    vec_mm = np.memmap(f\"{OUT_DIR}/vecs.f32\", dtype=\"float32\", mode=\"w+\", shape=(n, dim))\n",
    "    dur_mm = np.memmap(f\"{OUT_DIR}/durs.f32\", dtype=\"float32\", mode=\"w+\", shape=(n,))\n",
    "\n",
    "    string_ids = []\n",
    "\n",
    "    ptr = 0\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Get Embedding\"):\n",
    "        cut_batch = cut_list[i:i+batch_size]\n",
    "\n",
    "        texts = [c.supervisions[0].text if c.supervisions else \"\" for c in cut_batch]\n",
    "        durations = [c.duration for c in cut_batch]\n",
    "        string_ids.extend([c.id for c in cut_batch])\n",
    "\n",
    "        vec_np = get_sentence_embeddings(texts).astype(\"float32\")\n",
    "        B = len(cut_batch)\n",
    "\n",
    "        vec_mm[ptr:ptr+B] = vec_np\n",
    "        dur_mm[ptr:ptr+B] = durations\n",
    "        ptr += B\n",
    "\n",
    "    vec_mm.flush(); dur_mm.flush()\n",
    "\n",
    "    return vec_mm, dur_mm, string_ids\n",
    "\n",
    "def build_hnsw_index(vec_mm: np.memmap,\n",
    "                     dim: int = 384,\n",
    "                     m: int = 32,\n",
    "                     ef_c: int = 200,\n",
    "                     n_threads: int = mp.cpu_count(),\n",
    "                     out_path: str = \"cache_hnsw.faiss\"):\n",
    "\n",
    "    faiss.omp_set_num_threads(n_threads)\n",
    "    faiss.normalize_L2(vec_mm)\n",
    "\n",
    "    index = faiss.IndexHNSWFlat(dim, m)\n",
    "    index.hnsw.efConstruction = ef_c\n",
    "    index.metric_type = faiss.METRIC_INNER_PRODUCT\n",
    "\n",
    "    index.add(vec_mm)\n",
    "    faiss.write_index(index, os.path.join(OUT_DIR,out_path))\n",
    "    return os.path.join(OUT_DIR,out_path)\n",
    "\n",
    "def get_speaker_embedding_ids(ids, neighs, cuts):\n",
    "    \"\"\"\n",
    "    获取邻居的说话人ID\n",
    "    Returns:\n",
    "        speaker_embeddings: (batch_num, feature_dim)\n",
    "    \"\"\"\n",
    "    speaker_embeddings = []\n",
    "    for idx in neighs:\n",
    "        if idx == -1:\n",
    "            break\n",
    "        real_id = ids[idx]\n",
    "        cut_pth = cuts[real_id].recording.sources[0].source\n",
    "        audio, sr = librosa.load(cut_pth)\n",
    "        speaker_embeddings.append(get_speaker_embedding(audio, sr).flatten())\n",
    "\n",
    "    spk_emb_np = np.array(speaker_embeddings)\n",
    "    pc1 = PCA(n_components=1, svd_solver=\"auto\").fit_transform(spk_emb_np).ravel()\n",
    "    return np.argsort(pc1)\n",
    "\n",
    "def greedy_cluster(index_path: str,\n",
    "                   vec_mm: np.memmap,\n",
    "                   dur_mm: np.memmap,\n",
    "                   ids,\n",
    "                   cuts,\n",
    "                   bucket_min: int = 480,\n",
    "                   bucket_avg: int = 600,\n",
    "                   k_neigh: int = 256,\n",
    "                   ef_s: int = 96):\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    params = faiss.SearchParametersHNSW()\n",
    "    params.efSearch = ef_s\n",
    "\n",
    "    N = len(vec_mm)\n",
    "    assigned = bitarray(N)\n",
    "    assigned.setall(False)\n",
    "\n",
    "    order = np.argsort(-dur_mm)\n",
    "    buckets = []\n",
    "\n",
    "    for seed in tqdm(order, desc=\"Clustering (Optimized)\"):\n",
    "        if assigned[seed]:\n",
    "            continue\n",
    "\n",
    "        cluster = []\n",
    "        total_dur = 0\n",
    "\n",
    "        unassigned_indices_list = assigned.search(bitarray('0'))\n",
    "        unassigned_indices = np.fromiter(unassigned_indices_list, dtype=np.int64)\n",
    "\n",
    "\n",
    "        if len(unassigned_indices) > 0:\n",
    "            selector = faiss.IDSelectorArray(unassigned_indices)\n",
    "            params.sel = selector\n",
    "\n",
    "            _, neighs = index.search(vec_mm[seed : seed + 1], k_neigh, params=params)\n",
    "\n",
    "            #speaker_order = get_speaker_embedding_ids(ids, neighs[0].tolist(), cuts)\n",
    "            #print(speaker_order)\n",
    "\n",
    "            for idx in neighs[0]:\n",
    "                if idx == -1:\n",
    "                    break\n",
    "                if assigned[idx]:\n",
    "                    print(\"Warning: Already assigned index\", idx)\n",
    "                    continue\n",
    "\n",
    "                cluster.append(int(idx))\n",
    "                assigned[idx] = True\n",
    "                total_dur += dur_mm[idx]\n",
    "                if total_dur >= bucket_avg:\n",
    "                    break\n",
    "\n",
    "            if total_dur < bucket_min:\n",
    "                for i in cluster:\n",
    "                    assigned[i] = False\n",
    "            else:\n",
    "                total_dur = dur_mm[cluster].sum()\n",
    "                buckets.append((cluster, total_dur))\n",
    "\n",
    "    final_buckets = [b for b in buckets if b[1] >= bucket_min]\n",
    "    final_clusters = [c for c, _ in final_buckets]\n",
    "    final_duration = sum(sec for _, sec in final_buckets)\n",
    "\n",
    "    loss = 1 - final_duration / dur_mm.sum()\n",
    "    print(f\"桶数 {len(final_clusters)}, 最终时长 {final_duration:.2f}s, 总时长 {dur_mm.sum():.2f}s, 丢弃比例 {loss:.2%}\")\n",
    "\n",
    "    strategy = []\n",
    "    for cluster in final_clusters:\n",
    "        strategy.append([ids[i] for i in cluster])\n",
    "\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25e121040a3ae99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:46:26.001705Z",
     "start_time": "2025-07-17T03:42:03.747620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get Embedding: 100%|██████████| 1369/1369 [00:32<00:00, 42.47it/s]\n",
      "Clustering (Optimized): 100%|██████████| 136848/136848 [02:20<00:00, 971.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "桶数 1161, 最终时长 693642.51s, 总时长 735715.31s, 丢弃比例 5.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "mock_strategy = [\n",
    "    [\"common_voice_en_43199993-0\", \"common_voice_en_42736613-1\", \"common_voice_en_42798328-2\"],\n",
    "    [\"common_voice_en_43204215-3\", \"common_voice_en_42706055-4\", \"common_voice_en_43139615-5\"]\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "vec_mm, dur_mm, string_ids = build_feature(resampled_cuts)\n",
    "index_path = build_hnsw_index(vec_mm)\n",
    "real_strategy = greedy_cluster(index_path, vec_mm, dur_mm, string_ids, resampled_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e2d3ce5edad1ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:47:26.345595Z",
     "start_time": "2025-07-17T03:47:26.341492Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_newid_cutset(cutset: CutSet, start_id: int = 0) -> CutSet:\n",
    "    \"\"\"\n",
    "    Map the ids of a CutSet to a new id starting from start_id.\n",
    "    \"\"\"\n",
    "    new_cuts = []\n",
    "    for i, cut in enumerate(cutset):\n",
    "        new_cut = cut.with_id(f\"{start_id + i:06d}\")\n",
    "        new_cuts.append(new_cut)\n",
    "    return CutSet.from_cuts(new_cuts), start_id + len(new_cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d35bb307c93c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:47:29.071564Z",
     "start_time": "2025-07-17T03:47:29.038806Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped_cuts = from_strategy_to_cuts(resampled_cuts.to_eager(), real_strategy)\n",
    "grouped_cuts , new_amount = map_newid_cutset(grouped_cuts, start_id=prev_amount)\n",
    "grouped_cuts.to_jsonl(os.path.join(OUT_DIR, \"grouped_raw_cuts.jsonl\"))\n",
    "print(new_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419954e0b702973b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:47:31.462411Z",
     "start_time": "2025-07-17T03:47:31.449615Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_from_commonvoice_to_allaudios(one_cut, lang = \"en\"):\n",
    "    \"\"\"\n",
    "    Convert a single Commonvoice json record to a list of LongSpeech metadata.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    speakers = set()\n",
    "    total_dur = 0\n",
    "    transcripts = []\n",
    "    slices = []\n",
    "\n",
    "    for subcut in one_cut[\"tracks\"]:\n",
    "        total_dur += subcut[\"cut\"][\"duration\"]\n",
    "        full_pth = subcut[\"cut\"][\"recording\"][\"sources\"][0][\"source\"]\n",
    "        slices.append([subcut[\"cut\"][\"start\"], subcut[\"cut\"][\"duration\"]])\n",
    "        sources.append(full_pth.split(\"clips\")[-1])\n",
    "        [speakers.add(s[\"speaker\"]) for s in subcut[\"cut\"][\"supervisions\"] if s[\"speaker\"]]\n",
    "        transcript_param = \" \".join([s[\"text\"] for s in subcut[\"cut\"][\"supervisions\"] if s[\"text\"]])\n",
    "        if transcript_param != \"\":\n",
    "            transcripts.append(transcript_param)\n",
    "        else:\n",
    "            print(subcut)\n",
    "\n",
    "    return {\n",
    "        \"id\": one_cut[\"id\"],\n",
    "        \"source_ds\": \"CommonVoice\",\n",
    "        \"duration_sec\": total_dur,\n",
    "        \"audio_auto\": False,\n",
    "        \"text_auto\": False,\n",
    "        \"language\": lang,\n",
    "        \"num_speakers\": len(speakers),\n",
    "        \"num_switches\": len(transcripts),\n",
    "        \"slice\": slices,\n",
    "        \"transcribe\": \" \".join(transcripts),\n",
    "        \"components\": sources,\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_record(source_jsonl_path: str, target_jsonl_path: str, map_fn, lang: str):\n",
    "    with open(source_jsonl_path, \"r\", encoding=\"utf-8\") as src_f, \\\n",
    "         open(target_jsonl_path, \"a\", encoding=\"utf-8\") as tgt_f:\n",
    "        for line in src_f:\n",
    "            item = json.loads(line)\n",
    "            new_item = map_fn(item, lang)\n",
    "            tgt_f.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_audios_from_cutset(cutset, out_dir, num_jobs=1):\n",
    "    \"\"\"\n",
    "    Save audios from a CutSet to the specified directory.\n",
    "    \"\"\"\n",
    "    for cut in tqdm(cutset):\n",
    "        cut.save_audio(os.path.join(out_dir, f\"{cut.id}.wav\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73da044d6b28be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:48:09.821171Z",
     "start_time": "2025-07-17T03:47:34.637295Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:35<00:00, 17.58s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "convert_record(os.path.join(OUT_DIR, \"grouped_raw_cuts.jsonl\"),\n",
    "               os.path.join(OUT_DIR, OUT_FILE_NAME),\n",
    "               json_from_commonvoice_to_allaudios, lang)\n",
    "#save_audios_from_cutset(grouped_cuts, os.path.join(OUT_DIR, 'wavs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4c66ba1092b59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T03:49:03.069293Z",
     "start_time": "2025-07-17T03:49:03.063462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef with_new_features(cuts: CutSet, batch_size = 100) -> CutSet:\\n    cutset_list = cuts.split_lazy(OUT_DIR, batch_size)\\n    new_cutset_list = []\\n    for i, cutset in enumerate(tqdm(cutset_list, desc=\"Processing cuts\")):\\n        text_list = [cut.supervisions[0].text if cut.supervisions else \"\" for cut in cutset]\\n        id_list = [cut.id for cut in cutset]\\n        duration = [cut.duration for cut in cutset]\\n        semantic_np = get_sentence_embeddings(\\n            text_list\\n        )\\n\\n        updated_cuts = []\\n        for cut, embedding in zip(cutset, semantic_np):\\n            cut = cut.with_custom(\"semantic_emb\", embedding.tolist())  # 如果是 numpy array\\n            updated_cuts.append(cut)\\n\\n\\n        new_cutset_list.append(CutSet.from_cuts(updated_cuts))\\n    merged_cuts = combine(*new_cutset_list)\\n    return merged_cuts\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def with_new_features(cuts: CutSet, batch_size = 100) -> CutSet:\n",
    "    cutset_list = cuts.split_lazy(OUT_DIR, batch_size)\n",
    "    new_cutset_list = []\n",
    "    for i, cutset in enumerate(tqdm(cutset_list, desc=\"Processing cuts\")):\n",
    "        text_list = [cut.supervisions[0].text if cut.supervisions else \"\" for cut in cutset]\n",
    "        id_list = [cut.id for cut in cutset]\n",
    "        duration = [cut.duration for cut in cutset]\n",
    "        semantic_np = get_sentence_embeddings(\n",
    "            text_list\n",
    "        )\n",
    "\n",
    "        updated_cuts = []\n",
    "        for cut, embedding in zip(cutset, semantic_np):\n",
    "            cut = cut.with_custom(\"semantic_emb\", embedding.tolist())  # 如果是 numpy array\n",
    "            updated_cuts.append(cut)\n",
    "\n",
    "\n",
    "        new_cutset_list.append(CutSet.from_cuts(updated_cuts))\n",
    "    merged_cuts = combine(*new_cutset_list)\n",
    "    return merged_cuts\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
