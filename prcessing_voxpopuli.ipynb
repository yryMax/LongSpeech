{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T12:27:09.997605Z",
     "start_time": "2025-07-14T12:27:09.990088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lhotse import CutSet, RecordingSet, SupervisionSet, MonoCut\n",
    "from lhotse.cut import append_cuts\n",
    "from mylhotse.voxpopuli import prepare_voxpopuli\n",
    "import logging\n",
    "from functools import reduce\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a1954a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T12:27:14.707548Z",
     "start_time": "2025-07-14T12:27:14.704531Z"
    }
   },
   "outputs": [],
   "source": [
    "# directory paths to save audio and transcript files\n",
    "IN_DIR = \"../datasets/LongSpeechSource/voxpopuli\"\n",
    "# directory paths to save metadata and processed aduio files\n",
    "OUT_DIR = '../datasets/LongSpeech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a42d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_from_voxpopuli_to_allaudios(one_cut, lang = \"en\"):\n",
    "    \"\"\"\n",
    "    Convert a single LibriSpeech json record to a list of LongSpeech metadata.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    speakers = set()\n",
    "    total_dur = 0\n",
    "    transcripts = []\n",
    "    slices = []\n",
    "\n",
    "    for subcut in one_cut[\"tracks\"]:\n",
    "        total_dur += subcut[\"cut\"][\"duration\"]\n",
    "        full_pth = subcut[\"cut\"][\"recording\"][\"sources\"][0][\"source\"]\n",
    "        slices.append([subcut[\"cut\"][\"start\"], subcut[\"cut\"][\"duration\"]])\n",
    "        sources.append(full_pth.split(\"voxpopuli\")[-1])\n",
    "        [speakers.add(s[\"speaker\"]) for s in subcut[\"cut\"][\"supervisions\"] if s[\"speaker\"]]\n",
    "        transcript_param = \" \".join([s[\"text\"] for s in subcut[\"cut\"][\"supervisions\"] if s[\"text\"]])\n",
    "        if transcript_param != \"\":\n",
    "            transcripts.append(transcript_param)\n",
    "        else:\n",
    "            print(subcut)\n",
    "\n",
    "    return {\n",
    "        \"id\": one_cut[\"id\"],\n",
    "        \"source_ds\": \"voxpopuli\",\n",
    "        \"duration_sec\": total_dur,\n",
    "        \"audio_auto\": False,\n",
    "        \"text_auto\": False,\n",
    "        \"language\": lang,\n",
    "        \"num_speakers\": len(speakers),\n",
    "        \"num_switches\": len(one_cut[\"tracks\"]),\n",
    "        \"slice\": slices,\n",
    "        \"transcribe\": \" \".join(transcripts),\n",
    "        \"components\": sources,\n",
    "    }\n",
    "\n",
    "def pack_cuts_to_long_audio(\n",
    "    cuts: CutSet,\n",
    "    target_duration: float = 600.0,\n",
    "    starting_id: int = 0,\n",
    ") -> (CutSet, int):\n",
    "    final_long_cuts = []\n",
    "    buffer_cut = None\n",
    "\n",
    "    for cut in cuts:\n",
    "        buffer_cut = cut if buffer_cut is None else buffer_cut.append(cut)\n",
    "        if buffer_cut.duration >= target_duration:\n",
    "            final_long_cuts.append(buffer_cut.with_id(f\"{starting_id:06d}\"))\n",
    "            starting_id += 1\n",
    "            buffer_cut = None\n",
    "\n",
    "    return CutSet.from_cuts(final_long_cuts), starting_id\n",
    "\n",
    "def convert_record(source_jsonl_path: str, target_jsonl_path: str, map_fn, lang: str):\n",
    "    with open(source_jsonl_path, \"r\", encoding=\"utf-8\") as src_f, \\\n",
    "         open(target_jsonl_path, \"a\", encoding=\"utf-8\") as tgt_f:\n",
    "        for line in src_f:\n",
    "            item = json.loads(line)\n",
    "            new_item = map_fn(item, lang)\n",
    "            tgt_f.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_audios_from_cutset(cutset, out_dir, num_jobs=1):\n",
    "    \"\"\"\n",
    "    Save audios from a CutSet to the specified directory.\n",
    "    \"\"\"\n",
    "    for cut in tqdm(cutset):\n",
    "        cut.save_audio(os.path.join(out_dir, f\"{cut.id}.wav\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "config_setup",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T12:27:12.824160Z",
     "start_time": "2025-07-14T12:27:12.805294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8411\n"
     ]
    }
   ],
   "source": [
    "config = json.load(open(os.path.join(OUT_DIR, 'metadata.json')))\n",
    "AVG_DURATION = config['avg_duration']\n",
    "SAMPLE_RATE = config['sample_rate']\n",
    "OUT_FILE_NAME = config['source']\n",
    "prev_amount = config['amount']\n",
    "print(prev_amount)\n",
    "task = \"asr\"\n",
    "lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dd87ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Preparing recordings (this may take a few minutes)...\n",
      "Scanning audio files (*.ogg): 9572it [00:02, 3440.07it/s] \n",
      "INFO:root:Using pre-downloaded annotations ../datasets/LongSpeech/asr_en.tsv.gz\n",
      "100%|██████████| 412484/412484 [00:01<00:00, 223487.12it/s]\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20120911-0900-PLENARY-18-1516-5-8).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20130312-0900-PLENARY-11-1730-5-6).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20140917-0900-PLENARY-4-2368-5-10).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20160607-0900-PLENARY-4-3036-5-6).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20171129-0900-PLENARY-11-3568-5-1).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20180912-0900-PLENARY-3922-5-57).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20190718-0900-PLENARY-3972-5-31).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20191009-0900-PLENARY-3977-5-48).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20191126-0900-PLENARY-3986-5-25).\n",
      "  warnings.warn(\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/cut/mono.py:322: UserWarning: You are merging overlapping supervisions that have text transcripts. The result is likely to be unusable if you are going to train speech recognition models (cut id: 20200114-0900-PLENARY-3995-5-85).\n",
      "  warnings.warn(\n",
      "100%|██████████| 3011/3011 [36:06<00:00,  1.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount: 11422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "manifests = prepare_voxpopuli(corpus_dir=IN_DIR, output_dir=OUT_DIR, task=task, lang=lang, num_jobs=15)\n",
    "\n",
    "cuts = CutSet()\n",
    "for part in manifests.keys():\n",
    "    rs = manifests[part]['recordings']\n",
    "    ss = manifests[part]['supervisions']\n",
    "    cut = CutSet.from_manifests(recordings=rs, supervisions=ss)\n",
    "    cuts += cut\n",
    "\n",
    "spilted_cuts = cuts.trim_to_supervision_groups(max_pause=5).filter(lambda cut: cut.duration > 10).sort_by_recording_id().merge_supervisions(merge_policy=\"keep_first\")\n",
    "\n",
    "grouped_cuts, new_amount = pack_cuts_to_long_audio(spilted_cuts, target_duration=600.0, starting_id = prev_amount)\n",
    "grouped_cuts.to_jsonl(OUT_DIR + \"/grouped_cuts.jsonl\")\n",
    "convert_record(os.path.join(OUT_DIR, \"grouped_cuts.jsonl\"),\n",
    "               os.path.join(OUT_DIR, OUT_FILE_NAME),\n",
    "               json_from_voxpopuli_to_allaudios, lang)\n",
    "save_audios_from_cutset(grouped_cuts, os.path.join(OUT_DIR, 'wavs'))\n",
    "print(f\"Total amount: {new_amount}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
