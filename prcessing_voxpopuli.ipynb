{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lhotse import CutSet, RecordingSet, SupervisionSet, MonoCut\n",
    "from lhotse.cut import append_cuts\n",
    "from lhotse.recipes import prepare_voxpopuli, download_voxpopuli\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory paths to save audio and transcript files\n",
    "IN_DIR = \"../input/voxpopuli\"\n",
    "# directory paths to save metadata and processed aduio files\n",
    "OUT_DIR = \"../outputs/voxpopuli\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "if not os.path.exists(os.path.join(OUT_DIR, 'metadata.json')):\n",
    "    config = {\n",
    "        'avg_duration': 10.0,\n",
    "        'sample_rate': 16000,\n",
    "        'source': 'voxpopuli.jsonl',\n",
    "        'amount': 0\n",
    "    }\n",
    "    with open(os.path.join(OUT_DIR, 'metadata.json'), 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "config = json.load(open(os.path.join(OUT_DIR, 'metadata.json')))\n",
    "AVG_DURATION = config['avg_duration']\n",
    "SAMPLE_RATE = config['sample_rate']\n",
    "OUT_FILE_NAME = config['source']\n",
    "prev_amount = config['amount']\n",
    "print(prev_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_and_prepare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare VoxPopuli dataset for ASR task in English\n",
    "lang = 'en'\n",
    "task = 'asr'\n",
    "# Set to True to force re-download\n",
    "ForceDownload = True  \n",
    "\n",
    "if ForceDownload:\n",
    "    download_voxpopuli(target_dir=IN_DIR, subset=lang)\n",
    "\n",
    "manifests = prepare_voxpopuli(corpus_dir=IN_DIR, output_dir=OUT_DIR, task=task, lang=lang, num_jobs=15)\n",
    "\n",
    "logging.info(f\"RecordingSet: {manifests['train']['recordings']}\")\n",
    "logging.info(f\"SupervisionSet: {manifests['train']['supervisions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_manifests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'train' split for processing\n",
    "split = 'train'\n",
    "rs = manifests[split]['recordings']\n",
    "ss = manifests[split]['supervisions']\n",
    "\n",
    "ss_normalized = ss  # VoxPopuli provides 'normed_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_cuts",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = CutSet.from_manifests(recordings=rs, supervisions=ss_normalized)\n",
    "cuts.to_jsonl(os.path.join(OUT_DIR, 'raw_cuts.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grouping_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_group(\n",
    "    source_cuts: CutSet,\n",
    "    target_sec: int = 600,\n",
    "    min_segment_sec: int = 120,\n",
    "):\n",
    "    \"\"\"Split long cuts and filter short ones, returning a DataFrame and updated CutSet.\"\"\"\n",
    "    new_cuts = []\n",
    "    cut_data = []\n",
    "\n",
    "    for cut in source_cuts:\n",
    "        duration = cut.duration\n",
    "        if duration >= target_sec:\n",
    "            # Split into 600-second segments\n",
    "            num_segments = int(duration // target_sec)\n",
    "            for i in range(num_segments):\n",
    "                start = i * target_sec\n",
    "                end = (i + 1) * target_sec\n",
    "                new_id = f\"{cut.id}-split{i:03d}\"\n",
    "                new_cut = cut.truncate(offset=start, duration=target_sec)\n",
    "                new_cut = new_cut.with_id(new_id)\n",
    "                new_cuts.append(new_cut)\n",
    "                cut_data.append({\n",
    "                    'id': new_id,\n",
    "                    'duration': target_sec,\n",
    "                    'session_id': cut.id.split('-')[0],\n",
    "                    'segment_num': i,\n",
    "                    'speaker': cut.supervisions[0].speaker if cut.supervisions else 'unknown',\n",
    "                    'slice': [start, end]\n",
    "                })\n",
    "            # Handle remainder\n",
    "            remainder = duration % target_sec\n",
    "            if remainder >= min_segment_sec:\n",
    "                new_id = f\"{cut.id}-split{num_segments:03d}\"\n",
    "                new_cut = cut.truncate(offset=num_segments * target_sec, duration=remainder)\n",
    "                new_cut = new_cut.with_id(new_id)\n",
    "                new_cuts.append(new_cut)\n",
    "                cut_data.append({\n",
    "                    'id': new_id,\n",
    "                    'duration': remainder,\n",
    "                    'session_id': cut.id.split('-')[0],\n",
    "                    'segment_num': num_segments,\n",
    "                    'speaker': cut.supervisions[0].speaker if cut.supervisions else 'unknown',\n",
    "                    'slice': [num_segments * target_sec, duration]\n",
    "                })\n",
    "        elif duration >= min_segment_sec:\n",
    "            # Keep segments >= 2 minutes for splicing\n",
    "            new_cuts.append(cut)\n",
    "            cut_data.append({\n",
    "                'id': cut.id,\n",
    "                'duration': duration,\n",
    "                'session_id': cut.id.split('-')[0],\n",
    "                'segment_num': int(cut.id.split('-')[-1]),\n",
    "                'speaker': cut.supervisions[0].speaker if cut.supervisions else 'unknown',\n",
    "                'slice': None\n",
    "            })\n",
    "        # Discard segments < 2 minutes\n",
    "\n",
    "    logging.info(f\"Prepared {len(new_cuts)} cuts after splitting and filtering\")\n",
    "    return pd.DataFrame(cut_data), CutSet.from_cuts(new_cuts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33be590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_audio_groups(\n",
    "    df: pd.DataFrame,\n",
    "    target_sec: int = 600,\n",
    "    tol_sec: int = 60,\n",
    "    maximum_speakers: int = 3,\n",
    "    maximum_switches: int = 3,\n",
    "    max_segments: int = 3\n",
    "):\n",
    "    \"\"\"Group segments to reach ~600 seconds, with max 3 segments per group.\"\"\"\n",
    "    df_sorted = df.sort_values(['session_id', 'segment_num']).reset_index(drop=True)\n",
    "\n",
    "    groups = []\n",
    "    cur_group, cur_dur = [], 0.0\n",
    "    cur_speakers = set()\n",
    "    semantic_changes = 0\n",
    "    prev_session = None\n",
    "\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        seg_id = row['id']\n",
    "        dur = float(row['duration'])\n",
    "        session = row['session_id']\n",
    "        speaker = row['speaker']\n",
    "\n",
    "        # Skip segments already at target duration (from splitting)\n",
    "        if abs(dur - target_sec) < 1e-6:\n",
    "            groups.append(([seg_id], 1, 0))\n",
    "            continue\n",
    "\n",
    "        if cur_group and (cur_dur + dur > target_sec + tol_sec or len(cur_group) >= max_segments):\n",
    "            if cur_dur >= target_sec and len(cur_speakers) <= maximum_speakers and semantic_changes <= maximum_switches:\n",
    "                groups.append((cur_group, len(cur_speakers), semantic_changes))\n",
    "            cur_group, cur_dur = [], 0.0\n",
    "            cur_speakers, semantic_changes = set(), 0\n",
    "            prev_session = None\n",
    "\n",
    "        if prev_session is not None and session != prev_session:\n",
    "            semantic_changes += 1\n",
    "\n",
    "        cur_group.append(seg_id)\n",
    "        cur_dur += dur\n",
    "        cur_speakers.add(speaker)\n",
    "        prev_session = session\n",
    "\n",
    "    if cur_group and cur_dur >= target_sec and len(cur_speakers) <= maximum_speakers and semantic_changes <= maximum_switches:\n",
    "        groups.append((cur_group, len(cur_speakers), semantic_changes))\n",
    "\n",
    "    logging.info(f\"Created {len(groups)} groups for splicing\")\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply_strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df, processed_cuts = prepare_and_group(cuts)\n",
    "logging.info(f\"DataFrame columns: {source_df.columns.tolist()}\")\n",
    "real_strategy = build_audio_groups(source_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cuts_from_strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audios_from_cutset(cutset, out_dir, num_jobs=1):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for cut in tqdm(cutset):\n",
    "        cut.save_audio(os.path.join(out_dir, f'{cut.id}.wav'))\n",
    "\n",
    "def from_strategy_to_cuts(source_cuts, strategy: list, cut_info: pd.DataFrame, starting_cut_id=0):\n",
    "    target_cuts_list = []\n",
    "    i = starting_cut_id\n",
    "    custom_feature = {}\n",
    "    for cluster_ids, num_speaker, num_switch in strategy:\n",
    "        cutlist = [source_cuts[cut_id] for cut_id in cluster_ids if cut_id in source_cuts]\n",
    "        if not cutlist:\n",
    "            logging.warning(f\"No valid cuts found for cluster: {cluster_ids}\")\n",
    "            continue\n",
    "        new_cut = append_cuts(cutlist) if len(cutlist) > 1 else cutlist[0]\n",
    "        new_id = f'{i:06d}'\n",
    "        new_cut = new_cut.with_id(new_id)\n",
    "        target_cuts_list.append(new_cut)\n",
    "        # Get slice info from cut_info\n",
    "        slice_info = cut_info[cut_info['id'].isin(cluster_ids)]['slice'].iloc[0] if len(cluster_ids) == 1 else None\n",
    "        custom_feature[new_id] = {\n",
    "            'num_speakers': num_speaker,\n",
    "            'num_switches': num_switch,\n",
    "            'slice': slice_info\n",
    "        }\n",
    "        i += 1\n",
    "    return CutSet(target_cuts_list), custom_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_grouped_cuts",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_cuts, custom_feature = from_strategy_to_cuts(processed_cuts, real_strategy, source_df, starting_cut_id=prev_amount)\n",
    "tgt_cuts.to_jsonl(os.path.join(OUT_DIR, 'grouped_cuts.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def json_from_voxpopuli_to_allaudios(custom_feature, one_cut):\n",
    "    sources = []\n",
    "    total_dur = 0\n",
    "    transcripts = []\n",
    "\n",
    "    if 'tracks' in one_cut:\n",
    "        logging.info(f\"Processing MixedCut with ID {one_cut['id']}, tracks: {len(one_cut['tracks'])}\")\n",
    "        for subcut in one_cut['tracks']:\n",
    "            total_dur += subcut['cut']['duration']\n",
    "            full_pth = subcut['cut']['recording']['sources'][0]['source']\n",
    "            sources.append(full_pth.split('raw_audios')[-1])\n",
    "            transcripts.append(subcut['cut']['supervisions'][0]['text'] if subcut['cut']['supervisions'] else '')\n",
    "    else:\n",
    "        logging.info(f\"Processing MonoCut with ID {one_cut['id']}\")\n",
    "        total_dur = one_cut['duration']\n",
    "        full_pth = one_cut['recording']['sources'][0]['source']\n",
    "        sources.append(full_pth.split('raw_audios')[-1])\n",
    "        transcripts.append(one_cut['supervisions'][0]['text'] if one_cut['supervisions'] else '')\n",
    "\n",
    "    result = {\n",
    "        'id': one_cut['id'],\n",
    "        'source_ds': 'voxpopuli',\n",
    "        'duration_sec': total_dur,\n",
    "        'audio_auto': False,\n",
    "        'text_auto': False,\n",
    "        'num_speakers': custom_feature.get(one_cut['id'], {}).get('num_speakers', -1),\n",
    "        'num_switches': custom_feature.get(one_cut['id'], {}).get('num_switches', -1),\n",
    "        'transcribe': ' '.join([t for t in transcripts if t]),\n",
    "        'components': sources,\n",
    "    }\n",
    "\n",
    "    if custom_feature.get(one_cut['id'], {}).get('slice'):\n",
    "        result['slice'] = custom_feature[one_cut['id']]['slice']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_record(source_jsonl_path: str, target_jsonl_path: str, custom_feature, map_fn):\n",
    "    with open(source_jsonl_path, 'r', encoding='utf-8') as src_f, \\\n",
    "         open(target_jsonl_path, 'a', encoding='utf-8') as tgt_f:\n",
    "        for line in src_f:\n",
    "            item = json.loads(line)\n",
    "            new_item = map_fn(custom_feature, item)\n",
    "            tgt_f.write(json.dumps(new_item, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute_conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "convert_record(\n",
    "    os.path.join(OUT_DIR, 'grouped_cuts.jsonl'),\n",
    "    os.path.join(OUT_DIR, OUT_FILE_NAME),\n",
    "    custom_feature,\n",
    "    json_from_voxpopuli_to_allaudios\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_wavs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save audio files\n",
    "save_audios_from_cutset(tgt_cuts, os.path.join(OUT_DIR, 'wavs'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
