{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:39:13.533913Z",
     "start_time": "2025-07-11T12:39:03.361372Z"
    }
   },
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lhotse import CutSet, RecordingSet, SupervisionSet, MonoCut\n",
    "from lhotse.cut import append_cuts\n",
    "from lhotse.recipes import prepare_tedlium\n",
    "from util import *\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renyi/anaconda3/envs/cosyvoice2/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "IN_DIR = \"../datasets/LongSpeechSource/TEDLIUM_release-3\"\n",
    "OUT_DIR = '../datasets/LongSpeech'\n",
    "manifests = prepare_tedlium(tedlium_root=IN_DIR, output_dir=OUT_DIR, num_jobs=15)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cuts = CutSet()\n",
    "for part in manifests.keys():\n",
    "    rs = manifests[part]['recordings']\n",
    "    ss = manifests[part]['supervisions']\n",
    "    cut = CutSet.from_manifests(recordings=rs, supervisions=ss)\n",
    "    cuts += cut"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:23.224835Z",
     "start_time": "2025-07-11T11:39:23.180809Z"
    }
   },
   "cell_type": "code",
   "source": "#cuts = CutSet.from_jsonl(os.path.join('../datasets/LongSpeech', 'raw_ted_lium_cuts_head.jsonl'))",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:25.235510Z",
     "start_time": "2025-07-11T11:39:25.225920Z"
    }
   },
   "cell_type": "code",
   "source": "cuts = cuts.transform_text(lambda text: text.replace('<unk>', '').strip())",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:27.400066Z",
     "start_time": "2025-07-11T11:39:27.394716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trim_silence_from_cut(cut):\n",
    "    \"\"\"\n",
    "    根据 supervision 信息修剪单个 Cut 的两端空白。\n",
    "    如果没有 supervision，则认为整个 cut 都是空白，返回 None。\n",
    "    \"\"\"\n",
    "    if not cut.supervisions:\n",
    "        return None\n",
    "\n",
    "    speech_start = min(s.start for s in cut.supervisions)\n",
    "    speech_end = max(s.end for s in cut.supervisions)\n",
    "\n",
    "    new_duration = speech_end - speech_start\n",
    "\n",
    "    if new_duration <= 0:\n",
    "        return None\n",
    "\n",
    "    return cut.truncate(offset=speech_start, duration=new_duration)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:29.363030Z",
     "start_time": "2025-07-11T11:39:29.353317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cuts = cuts.map (\n",
    "    lambda cut: trim_silence_from_cut(cut)\n",
    ").filter(lambda cut: cut is not None)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:48:51.295896Z",
     "start_time": "2025-07-11T11:48:51.292691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunked_cuts = cuts.cut_into_windows(\n",
    "    duration=600,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:59:37.569874Z",
     "start_time": "2025-07-11T12:59:37.565901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pack_cuts_to_long_audio(\n",
    "    cuts: CutSet,\n",
    "    target_duration: float = 600.0,\n",
    "    staring_id =  0,\n",
    ") -> CutSet:\n",
    "    final_long_cuts = []\n",
    "    buffer_cut = None\n",
    "\n",
    "    for cut in cuts:\n",
    "        buffer_cut = buffer_cut.append(cut) if buffer_cut else cut\n",
    "\n",
    "        while buffer_cut.duration >= target_duration:\n",
    "            new_chunk = buffer_cut.truncate(offset=0, duration=target_duration)\n",
    "            final_long_cuts.append(new_chunk.with_id(f\"{staring_id:06d}\"))\n",
    "            staring_id += 1\n",
    "            buffer_cut = buffer_cut.truncate(offset=target_duration)\n",
    "\n",
    "    return CutSet.from_cuts(final_long_cuts)\n"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:59:57.321038Z",
     "start_time": "2025-07-11T12:59:57.285967Z"
    }
   },
   "cell_type": "code",
   "source": "sliced_cuts = pack_cuts_to_long_audio(chunked_cuts, target_duration=600.0)",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:59:58.771845Z",
     "start_time": "2025-07-11T12:59:58.735367Z"
    }
   },
   "cell_type": "code",
   "source": "sliced_cuts.to_jsonl(OUT_DIR + \"/grouped_cuts.jsonl\")",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:51:28.066833Z",
     "start_time": "2025-07-11T12:51:28.057232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def json_from_tedlium_to_allaudios(one_cut):\n",
    "    \"\"\"\n",
    "    Convert a single LibriSpeech json record to a list of LongSpeech metadata.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    total_dur = 0\n",
    "    transcripts = []\n",
    "    slices = []\n",
    "    for subcut in one_cut[\"tracks\"]:\n",
    "        total_dur += subcut[\"cut\"][\"duration\"]\n",
    "        full_pth = subcut[\"cut\"][\"recording\"][\"sources\"][0][\"source\"]\n",
    "        slices.append([subcut[\"cut\"][\"start\"], subcut[\"cut\"][\"duration\"]])\n",
    "        sources.append(full_pth.split(\"TEDLIUM_release-3\")[-1])\n",
    "        transcript_param = \" \".join([s[\"text\"] for s in subcut[\"cut\"][\"supervisions\"] if s[\"text\"]])\n",
    "        transcripts.append(restore_punctuation(transcript_param))\n",
    "\n",
    "    return {\n",
    "        \"id\": one_cut[\"id\"],\n",
    "        \"source_ds\": \"tedlium\",\n",
    "        \"duration_sec\": total_dur,\n",
    "        \"audio_auto\": False,\n",
    "        \"test_auto\": False,\n",
    "        \"num_speakers\": len(sources),\n",
    "        \"num_switches\": len(sources),\n",
    "        \"slice\": slices,\n",
    "        \"transcribe\": \" \".join(transcripts),\n",
    "        \"components\": sources,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:51:30.424959Z",
     "start_time": "2025-07-11T12:51:30.421798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_record(source_jsonl_path: str, target_jsonl_path: str, map_fn):\n",
    "    with open(source_jsonl_path, \"r\", encoding=\"utf-8\") as src_f, \\\n",
    "         open(target_jsonl_path, \"a\", encoding=\"utf-8\") as tgt_f:\n",
    "        for line in src_f:\n",
    "            item = json.loads(line)\n",
    "            new_item = map_fn(item)\n",
    "            tgt_f.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:51:37.510177Z",
     "start_time": "2025-07-11T12:51:33.606129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "convert_record(os.path.join(OUT_DIR, \"tmp.jsonl\"),\n",
    "               os.path.join(OUT_DIR, \"final.jsonl\"),\n",
    "               json_from_tedlium_to_allaudios)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_audios_from_cutset(sliced_cuts, os.path.join(OUT_DIR, 'wavs'))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
