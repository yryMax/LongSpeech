{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:39:13.533913Z",
     "start_time": "2025-07-11T12:39:03.361372Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lhotse import CutSet, RecordingSet, SupervisionSet, MonoCut\n",
    "from lhotse.cut import append_cuts\n",
    "from lhotse.recipes import prepare_tedlium\n",
    "from util import *\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing train split...\n",
      "Scanning audio files (*.sph): 2351it [00:14, 159.70it/s]\n",
      "INFO:root:Processing dev split...\n",
      "Scanning audio files (*.sph): 8it [00:00, 57.81it/s]\n",
      "INFO:root:Processing test split...\n",
      "Scanning audio files (*.sph): 11it [00:00, 60.44it/s]\n"
     ]
    }
   ],
   "source": [
    "IN_DIR = \"../datasets/LongSpeechSource/TEDLIUM_release-3\"\n",
    "OUT_DIR = '../datasets/LongSpeech'\n",
    "manifests = prepare_tedlium(tedlium_root=IN_DIR, output_dir=OUT_DIR, num_jobs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5290\n"
     ]
    }
   ],
   "source": [
    "config = json.load(open(os.path.join(OUT_DIR, 'metadata.json')))\n",
    "AVG_DURATION = config['avg_duration']\n",
    "SAMPLE_RATE = config['sample_rate']\n",
    "OUT_FILE_NAME = config['source']\n",
    "prev_amount = config['amount']\n",
    "print(prev_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = CutSet()\n",
    "for part in manifests.keys():\n",
    "    rs = manifests[part]['recordings']\n",
    "    ss = manifests[part]['supervisions']\n",
    "    cut = CutSet.from_manifests(recordings=rs, supervisions=ss)\n",
    "    cuts += cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:23.224835Z",
     "start_time": "2025-07-11T11:39:23.180809Z"
    }
   },
   "outputs": [],
   "source": [
    "#cuts = CutSet.from_jsonl(os.path.join('../datasets/LongSpeech', 'raw_ted_lium_cuts_head.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:25.235510Z",
     "start_time": "2025-07-11T11:39:25.225920Z"
    }
   },
   "outputs": [],
   "source": [
    "cuts = cuts.transform_text(lambda text: text.replace('<unk>', '').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:27.400066Z",
     "start_time": "2025-07-11T11:39:27.394716Z"
    }
   },
   "outputs": [],
   "source": [
    "def trim_silence_from_cut(cut):\n",
    "    \"\"\"\n",
    "    根据 supervision 信息修剪单个 Cut 的两端空白。\n",
    "    如果没有 supervision，则认为整个 cut 都是空白，返回 None。\n",
    "    \"\"\"\n",
    "    if not cut.supervisions:\n",
    "        return None\n",
    "\n",
    "    speech_start = min(s.start for s in cut.supervisions)\n",
    "    speech_end = max(s.end for s in cut.supervisions)\n",
    "\n",
    "    new_duration = speech_end - speech_start\n",
    "\n",
    "    if new_duration <= 0:\n",
    "        return None\n",
    "\n",
    "    return cut.truncate(offset=speech_start, duration=new_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T11:39:29.363030Z",
     "start_time": "2025-07-11T11:39:29.353317Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/renyi/miniconda3/envs/test3/lib/python3.10/site-packages/lhotse/lazy.py:683: UserWarning: A lambda was passed to LazyMapper: it may prevent you from forking this process. If you experience issues with num_workers > 0 in torch.utils.data.DataLoader, try passing a regular function instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cuts = cuts.map (\n",
    "    lambda cut: trim_silence_from_cut(cut)\n",
    ").filter(lambda cut: cut is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_cuts = cuts.cut_into_windows(\n",
    "    duration=600,\n",
    "    hop=600,       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:59:37.569874Z",
     "start_time": "2025-07-11T12:59:37.565901Z"
    }
   },
   "outputs": [],
   "source": [
    "def pack_cuts_to_long_audio(\n",
    "    cuts: CutSet,\n",
    "    target_duration: float = 600.0,\n",
    "    staring_id =  0,\n",
    ") -> CutSet:\n",
    "    final_long_cuts = []\n",
    "    buffer_cut = None\n",
    "\n",
    "    for cut in cuts:\n",
    "        buffer_cut = buffer_cut.append(cut) if buffer_cut else cut\n",
    "\n",
    "        while buffer_cut.duration >= target_duration:\n",
    "            new_chunk = buffer_cut.truncate(offset=0, duration=target_duration)\n",
    "            final_long_cuts.append(new_chunk.with_id(f\"{staring_id:06d}\"))\n",
    "            staring_id += 1\n",
    "            buffer_cut = buffer_cut.truncate(offset=target_duration)\n",
    "\n",
    "    return CutSet.from_cuts(final_long_cuts), staring_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:59:57.321038Z",
     "start_time": "2025-07-11T12:59:57.285967Z"
    }
   },
   "outputs": [],
   "source": [
    "sliced_cuts, new_amount = pack_cuts_to_long_audio(chunked_cuts, target_duration=600.0, staring_id = prev_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8411"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:59:58.771845Z",
     "start_time": "2025-07-11T12:59:58.735367Z"
    }
   },
   "outputs": [],
   "source": [
    "sliced_cuts.to_jsonl(OUT_DIR + \"/grouped_cuts.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:51:28.066833Z",
     "start_time": "2025-07-11T12:51:28.057232Z"
    }
   },
   "outputs": [],
   "source": [
    "def json_from_tedlium_to_allaudios(one_cut):\n",
    "    \"\"\"\n",
    "    Convert a single LibriSpeech json record to a list of LongSpeech metadata.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    total_dur = 0\n",
    "    transcripts = []\n",
    "    slices = []\n",
    "    for subcut in one_cut[\"tracks\"]:\n",
    "        total_dur += subcut[\"cut\"][\"duration\"]\n",
    "        full_pth = subcut[\"cut\"][\"recording\"][\"sources\"][0][\"source\"]\n",
    "        slices.append([subcut[\"cut\"][\"start\"], subcut[\"cut\"][\"duration\"]])\n",
    "        sources.append(full_pth.split(\"TEDLIUM_release-3\")[-1])\n",
    "        transcript_param = \" \".join([s[\"text\"] for s in subcut[\"cut\"][\"supervisions\"] if s[\"text\"]])\n",
    "        if transcript_param != \"\":\n",
    "            transcripts.append(restore_punctuation(transcript_param))\n",
    "        else:\n",
    "            print(subcut)\n",
    "\n",
    "    return {\n",
    "        \"id\": one_cut[\"id\"],\n",
    "        \"source_ds\": \"tedlium\",\n",
    "        \"duration_sec\": total_dur,\n",
    "        \"audio_auto\": False,\n",
    "        \"test_auto\": False,\n",
    "        \"num_speakers\": len(sources),\n",
    "        \"num_switches\": len(sources),\n",
    "        \"slice\": slices,\n",
    "        \"transcribe\": \" \".join(transcripts),\n",
    "        \"components\": sources,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:51:30.424959Z",
     "start_time": "2025-07-11T12:51:30.421798Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_record(source_jsonl_path: str, target_jsonl_path: str, map_fn):\n",
    "    with open(source_jsonl_path, \"r\", encoding=\"utf-8\") as src_f, \\\n",
    "         open(target_jsonl_path, \"a\", encoding=\"utf-8\") as tgt_f:\n",
    "        for line in src_f:\n",
    "            item = json.loads(line)\n",
    "            new_item = map_fn(item)\n",
    "            tgt_f.write(json.dumps(new_item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T12:51:37.510177Z",
     "start_time": "2025-07-11T12:51:33.606129Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cut': {'id': '832a3861-3cee-4c3b-8a22-b3928198bac4', 'start': 611.52, 'duration': 2.5, 'channel': 0, 'supervisions': [], 'recording': {'id': 'Beardyman_2013', 'sources': [{'type': 'file', 'channels': [0], 'source': '../datasets/LongSpeechSource/TEDLIUM_release-3/legacy/train/sph/Beardyman_2013.sph'}], 'sampling_rate': 16000, 'num_samples': 11306446, 'duration': 706.652875, 'channel_ids': [0]}, 'type': 'MonoCut'}, 'type': 'MonoCut', 'offset': 0.0}\n"
     ]
    }
   ],
   "source": [
    "convert_record(os.path.join(OUT_DIR, \"grouped_cuts.jsonl\"),\n",
    "               os.path.join(OUT_DIR, OUT_FILE_NAME),\n",
    "               json_from_tedlium_to_allaudios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audios_from_cutset(cutset, out_dir, num_jobs=1):\n",
    "    \"\"\"\n",
    "    Save audios from a CutSet to the specified directory.\n",
    "    \"\"\"\n",
    "    for cut in tqdm(cutset):\n",
    "        cut.save_audio(os.path.join(out_dir, f\"{cut.id}.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_audios_from_cutset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_audios_from_cutset(sliced_cuts, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(OUT_DIR, \u001b[39m'\u001b[39m\u001b[39mwavs\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_audios_from_cutset' is not defined"
     ]
    }
   ],
   "source": [
    "save_audios_from_cutset(sliced_cuts, os.path.join(OUT_DIR, 'wavs'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
